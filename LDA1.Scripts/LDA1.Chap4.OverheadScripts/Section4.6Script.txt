Hello and welcome back. My name is Brian Hartman and I am an associate professor at Brigham Young University.
In the frequentist interpretation, one treats the vector of parameters theta as fixed yet unknown, whereas the outcomes X are realizations of random variables. With Bayesian statistical models, one views both the model parameters and the data as random variables. Once the parameters have a distribution, we can use probability tools to reflect and analyze this uncertainty about the parameters theta. For notation, we will think about theta as a random vector and let pi of theta denote the distribution of possible outcomes.
There are several advantages of the Bayesian approach. One can describe the entire distribution of parameters conditional on the data. This allows one, for example, to provide probability statements regarding the likelihood of parameters. This approach allows analysts to blend information known from other sources with the data in a coherent manner. This topic is developed in detail in the credibility chapter. The Bayesian approach provides for a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, require an approach to estimating variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. It is also convenient for explaining results to consumers of the data analysis.
There are two main parts to the Bayesian model. The prior distribution is your belief about the parameters before any data is collected. This is a great place to systematically include expert opinion about the parameters or the results from previous studies. The model distribution, or the likelihood, is the contribution of the data.
The prior and likelihood are combined using Bayes’ rule to get the posterior distribution. This is one of the main benefits of the Bayesian paradigm, you have a belief, gather some data, update that belief, gather more data. You can continue that iterative process as long as you would like. There is a saying that today’s posterior is tomorrow’s prior. Once you have the posterior distribution of the parameters of interest, you can summarize those parameters using a credible interval. It sounds just like a confidence interval, but is so much better because rather than having to say “we are 95% confident that the true parameter falls between a and b” you can say what you really want to say “there is a 0.95 probability that the parameter is between a and b.”
How do we calculate the posterior distribution? As mentioned on the previous slide, the posterior distribution is equal to the likelihood times the prior over the marginal. Notice that the denominator does not depend on theta, the parameter of interest. Therefore, the posterior distribution is proportional to the likelihood times the prior. That quantity is sometimes called the kernel. If the kernel is proportional to a known distribution, then we know the exact posterior distribution. In most realistic cases, we won’t know the closed-form of the posterior distribution and will need to simulate it using Markov chain Monte Carlo, or MCMC. Additionally, sometimes we can use normal approximations to make the MCMC more efficient.
One conjugate family which allows us to find the closed-form of the posterior is the Poisson-Gamma family. If your data is Poisson Lambda distributed, and you assume that lambda has a gamma alpha theta prior, then the posterior distribution of lambda is also gamma distributed with alpha equal to the alpha from the prior plus the sum of the x’s and theta equal to 1 over the sample size plus one over the theta from the prior.
