Hello and welcome to chapter 4. My name is Brian Hartman and I am an associate professor at Brigham Young University.
There are times when you do not want to or cannot make strong assumptions about the distribution of a quantity of interest. In those situations, we use nonparametric estimation, sometimes called empirical estimation. All we need to assume is that the observations come from a random sample, or are independent and identically distributed, or iid. We do not assume any specific cdf F.
Some of the simplest nonparametric estimators are the moment estimators. The kth raw moment is the expected value of X to the k. We estimate that moment using the sample average of x to the k. The kth central moment is the expected value of the quantity x minus the overall population mean mu to the k. That is estimated by the sample average of the kth power of the difference between each observation and the sample mean.
With no parametric assumptions about the distribution, we can approximate it through the empirical cumulative distribution function, which is simply the proportion of observations less than or equal to x for each x. When the random variable is discrete, we can also estimate the pmf by using the proportion of the sample equal to x.
In this toy example, we can estimate the mean by using the sample mean and get 19.7. We can also estimate the second central moment and get 31.01.
This plot shows the empirical cumulative distribution function of the toy example. For each x, this is the proportion of samples less than or equal to x.
Back to our toy example, the median can be any value between 20 and 23, though many software packages use the average, in this case 21.5. The smoothed empirical percentile uses linear interpolation to estimate the quantiles between samples.
To see how this works, we will find the 50th and 20th quantiles in the toy example. The computational details are on the slide, but heuristically, the 50th quantile is exactly between the 5th and 6th observations while the 20th quantile is closer to the 2nd observation than the 3rd, though in this case it does not matter because those two observations are the same.
When the data is discrete, we can estimate the probability mass function by the proportion of observations at each value. Alternatively, you can group the observations into intervals and then estimate the probability mass function for each of those intervals using the sample proportions.
Because empirically estimated pmfs can be rather jagged and variable, they can be smoothed using kernel density estimators. First, you pick a bandwidth. If we are estimating the probability mass function at x, any observation within b of x will contribute to the probability mass at x. The larger the bandwidth, the smoother the function will be. Additionally, the uniform kernel density estimator is an asymptotically unbiased estimator of f of x.
More generally, you can use many different types of kernels to estimate the density. Some commonly used examples are described on the slide here. Most software defaults to the Gaussian kernel, but is easily changed.
We apply the kernel density estimator to the distribution function like this. For illustration, the uniform kernel will work like this. 
Observations can be grouped into intervals. The empirical CDF is still defined at the boundaries in the usual way. Within each interval, we can use the ogive which is the same linear approximation we used before.
One interesting thing about the ogive is its derivative is called the histogram and is the same as the histograms that we plot to get a basic understanding of the data. 
