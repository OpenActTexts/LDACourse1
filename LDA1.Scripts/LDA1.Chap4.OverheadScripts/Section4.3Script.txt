Hello and welcome back. My name is Brian Hartman and I am an associate professor at Brigham Young University.
One important type of inference is to select one of two candidate models, where one model (reduced model) is a special case of the other model (full model). In a Likelihood Ratio Test, we conduct the following hypothesis test: H0: Reduced model is correct against H1: Full model is correct.
To conduct the Likelihood Ratio Test we first determine the maximum likelihood estimator for full model, theta hat full. Then we assume that p restrictions are placed on the parameters of the full model to create the reduced model; determine the maximum likelihood estimator for the reduced model, theta hat reduced. Two times the difference in log-likelihood of the full and reduced models is the likelihood ratio or LRT. Under the null hypothesis, the likelihood ratio has a chi-square distribution with degrees of freedom equal to p. Critical value is a quantile from a chi-square distribution with degrees of freedom equal to p - If LRT is large relative to the critical value, then we reject the reduced model in favor of the full model.
The following statistics can be used when comparing several candidate models that are not necessarily nested (as in the Likelihood Ratio Test). One picks the model that maximizes the criterion. Note that these are one version of the criterion, other versions may be multiplied by two or may minimize a negative number, but the relative order is what matter. Akaikeâ€™s Information Criterion (AIC) is the log-likelihood minus the number of parameters in the model. The number of parameters term is a penalty for the complexity of the model. Other things equal, a more complex model means more parameters, resulting in a smaller value of the criterion. Similarly, to use a more complex model, there must be a significant increase in the loglikelihood. Bayesian Information Criterion (BIC) is the loglikelihood minus 0.5 times the number of parameters times the number of observations. This measure gives greater weight to the number of parameters, resulting in a larger penalty. Other things being equal, BIC will suggest a simpler model than AIC.
As mentioned in the previous slide, here is an alternative version of AIC and BIC where you minimize the criterion. It is simply the AIC and BIC values from the previous slide multiplied by negative two. 
