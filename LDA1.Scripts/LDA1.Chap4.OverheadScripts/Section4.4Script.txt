Hello and welcome back. My name is Brian Hartman and I am an associate professor at Brigham Young University.
An outcome is potentially truncated when the availability of an observation depends on the outcome. In insurance, it is common for observations to be truncated from the left (or below) at d when the amount observed is Y which is equal to x minus d only when x is greater than or equal to d. In this case, d may represent the deductible associated with an insurance coverage. If the insured loss is less than the deductible, then the insurer does not observe the loss. If the loss exceeds the deductible, then the excess X minus d is the claim that the insurer covers. Observations may also be truncated from the right (or above) at d when the amount observed is x only when x is less than d. Classic examples of truncation from the right include X as a measure of distance of a star. When the distance exceeds a certain level d, the star is no longer observable.
This slide can look intimidating, I am sorry about that. Letâ€™s talk through it. The probability of an observation ending up in the jth interval is just the probability that it is less than or equal to the top of the interval, F of c sub j, minus the probability that it is less than or equal to the bottom of the interval, F of c sub j minus 1. The pmf just writes those formulas more generally. The likelihood is equal to the product of all the probabilities and the log-likelihood is just the natural log of the likelihood, or the sum of the log of the probabilities. 
Back to censoring, suppose that X represents a loss due to an insured event and that u is a known censoring point. If observations are censored from the right (or from above), then we observe Y equals the minimum of X and u and delta u is the indicator if x is greater than or equal to u. If censoring occurs so that delta u equals 1, then X is greater than or equal to u and the likelihood is 1 minus F of u. If censoring does not occur so that delta u equals 0, then X is less than c sub u and the likelihood is f(y). In notation, the likelihood is at the bottom of the slide.
Now that we have the likelihood for a single observation, consider a random sample of size n with a set of potential censoring times. Then the joint likelihood is just the product of the individual likelihoods, or the pdf of y sub I for all the uncensored observations times one minus the cdf of u sub i for all the censored observations. Again, the log likelihood is simply the natural log of the likelihood. 
Truncated data are handled in likelihood inference via conditional probabilities. Adjust the likelihood contribution by dividing by the probability that the variable was observed. Summarizing, we have the following contributions to the likelihood for six types of outcomes in the table at the bottom of the slide.
To combine the likelihoods, we simply multiply each of the terms from our sample. We can then take the derivative of the loglikelihood and set it equal to zero to find the maximum likelihood estimate.
