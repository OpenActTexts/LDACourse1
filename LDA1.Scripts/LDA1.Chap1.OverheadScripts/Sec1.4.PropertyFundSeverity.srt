1
00:00:01,750 --> 00:00:06,609
Welcome back to the tutorial on an Introduction
to Loss Data Analytics.

2
00:00:06,609 --> 00:00:13,650
Hello, my name is Yvonne Cheuh. In this video,
I am going to demonstrate analytics of severity

3
00:00:13,650 --> 00:00:18,210
distributions for the Wisconsin Property Fund.

4
00:00:18,210 --> 00:00:24,960
OVERHEAD 1 - Severity Distribution (2010)

5
00:00:24,960 --> 00:00:31,730
In the prior video on frequency of claims,
we learned that of the 1110 policyholders

6
00:00:31,730 --> 00:00:44,329
in 2010, 707 did not experience any claims.
This means that 403, which is 1110 minus 707,

7
00:00:44,329 --> 00:00:47,050
policyholders had at least one claim.

8
00:00:47,050 --> 00:00:54,570
When examining the severity, or the amount,
of claims, there are at least two basic approaches.

9
00:00:54,570 --> 00:00:59,110
One approach is to follow each claim and use
this information to get a sense of the distribution.

10
00:00:59,110 --> 00:01:05,089
Another approach, taken here, is to follow
each policyholder. The drawback of following

11
00:01:05,089 --> 00:01:09,890
a policyholder is that we typically no longer
have information about individual claims,

12
00:01:09,890 --> 00:01:14,430
only the sum of claims attributable to a policyholder.

13
00:01:14,430 --> 00:01:20,540
To make information among policyholders comparable,
it is customary to examine the average claim

14
00:01:20,540 --> 00:01:27,220
for each policyholder. To illustrate, 209
policyholders had only one claim. Here, the

15
00:01:27,220 --> 00:01:32,909
average claim is the single recorded claim.
In contrast, for example the average claim

16
00:01:32,909 --> 00:01:38,610
for the Madison Metropolitan School District
is the average over 209 different recorded

17
00:01:38,610 --> 00:01:40,920
events.

18
00:01:40,920 --> 00:01:47,040
With this as background, the table displayed
shows summary statistics for the 403 average

19
00:01:47,040 --> 00:01:54,810
claims. From this table, one immediately notices
that the maximum claim is over $12 million

20
00:01:54,810 --> 00:02:01,380
USD. This unusually large amount is from a
real event (a flood in a Milwaukee courthouse)

21
00:02:01,380 --> 00:02:08,270
and is very financially important to the fund.
In other areas of statistics, one might think

22
00:02:08,270 --> 00:02:14,330
of this as an "outlier" and so a candidate
of removal from the rest of the data set.

23
00:02:14,330 --> 00:02:19,720
However, in insurance statistics, unusually
large events and skewed distributions are

24
00:02:19,720 --> 00:02:22,780
common, not the exception.

25
00:02:22,780 --> 00:02:28,250
For example, take a look at the distribution
plotted on the left-hand side. This histrogram

26
00:02:28,250 --> 00:02:36,300
is not really helpful for the analyst because
it is dominated by the extreme large value,

27
00:02:36,300 --> 00:02:43,980
causing all the observations to be clustered
together on the left. One approach is to remove

28
00:02:43,980 --> 00:02:50,430
this large point, at least for plotting purposes.
However, this does not help as other observations

29
00:02:50,430 --> 00:02:56,990
take on a similar role. As an alternative,
it is common to plot logarithmic values, as

30
00:02:56,990 --> 00:03:02,300
done in the right-hand panel. This graphs
gives use a much better sense of the severity

31
00:03:02,300 --> 00:03:03,670
distribution.

32
00:03:03,670 --> 00:03:09,590
OVERHEAD 2 - Claims Severity - R Code

33
00:03:09,590 --> 00:03:14,540
So that you can replicate these results, here
is a look at the R code.

34
00:03:14,540 --> 00:03:20,110
As with our frequency analysis, the first
two lines show how to bring in the data and

35
00:03:20,110 --> 00:03:27,870
produce a data frame based on only 2010 information.
The third line creates a new data frame, InsamplePos2010,

36
00:03:27,870 --> 00:03:35,910
that takes a subset of policyholders with
a positive average severity. You can check

37
00:03:35,910 --> 00:03:43,210
that there are 403 policyholders with the
"length" command. The "summary" command produces

38
00:03:43,210 --> 00:03:45,540
the table of summary statistics.

39
00:03:45,540 --> 00:03:51,629
The rest of the code produces the two histograms
that we looked at. This is very handy because

40
00:03:51,629 --> 00:03:59,959
we often want to compare phenomena graphically.
The "mfrow" syntax produces a figure with

41
00:03:59,959 --> 00:04:03,920
1 row and 2 columns.

42
00:04:03,920 --> 00:04:09,670
OVERHEAD 3 - Claim Outcomes and Coverage by
Year

43
00:04:09,670 --> 00:04:16,630
We have seen analytics of frequency and severity
for 2010, so now let's take a look at trends

44
00:04:16,630 --> 00:04:24,500
over years. One could produce distributions
over the years but, to begin, we look to simpler

45
00:04:24,500 --> 00:04:32,100
averages. In the following display, we also
include the coverage. This plus the number

46
00:04:32,100 --> 00:04:39,310
of policyholders gives a sense of the overall
exposure of the fund.

47
00:04:39,310 --> 00:04:45,190
From the table, we see that the average frequency
is stable over the years, certainly in comparison

48
00:04:45,190 --> 00:04:50,960
with the severity. As we might guess from
the analysis of a single year for the severity,

49
00:04:50,960 --> 00:04:58,900
it tends to be volatile and is sensitive to
the impact of potentially large obsevations.

50
00:04:58,900 --> 00:05:08,170
Both exposure measures indicate stability
over this time period. The coverage is slightly

51
00:05:08,170 --> 00:05:12,660
increasing and the number of policyholders
slightly decreasing.

52
00:05:12,660 --> 00:05:20,940
OVERHEAD 4 - Analysis by Year - R Code

53
00:05:20,940 --> 00:05:26,830
Here is some R code that produces the results.
This uses the "summaryBy" function in the

54
00:05:26,830 --> 00:05:33,470
package "doBy". Although a lot of this course
is learning by doing combined with replicating

55
00:05:33,470 --> 00:05:38,729
examples, you will also want to get into the
habit of looking up the documentation for

56
00:05:38,729 --> 00:05:45,190
functions that you might use. This function
also uses a function internally that we created,

57
00:05:45,190 --> 00:05:48,200
labelled as "FUN1".

58
00:05:48,200 --> 00:05:52,770
You will see more code like this as we go
forward, so I won't go over all the details

59
00:05:52,770 --> 00:06:01,830
now. But, note that as a result, we produced
the data frame "Table1In". Then, we used the

60
00:06:01,830 --> 00:06:08,270
"names" function to give lables for each column
- this is very handy way of keeping track

61
00:06:08,270 --> 00:06:10,960
of elements.

62
00:06:10,960 --> 00:06:17,600
OVERHEAD 5 - Claim Frequency and Severity,
Deductibles, and Coverage

63
00:06:17,600 --> 00:06:25,500
For a different look at the 2006-2010 data,
this table summarizes the distribution of

64
00:06:25,500 --> 00:06:33,400
our two outcomes, frequency and claims amount.
In each case, the average exceeds the median,

65
00:06:33,400 --> 00:06:38,199
suggesting that the two distributions are
right-skewed. In addition, the table summarizes

66
00:06:38,199 --> 00:06:45,690
our continuous rating variables, coverage
and deductible amount. The table also suggests

67
00:06:45,690 --> 00:06:50,500
that these variables also have right-skewed
distributions.

68
00:06:50,500 --> 00:06:58,410
OVERHEAD 6 - Claim Frequency and Severity,
Deductibles, and Coverage - R Code

69
00:06:58,410 --> 00:07:04,640
Here is the R code used to produce this table.
Note that again we used the "SummaryBy" function

70
00:07:04,640 --> 00:07:10,460
even though there way "By" part - this was
done just so that you can see another application

71
00:07:10,460 --> 00:07:16,320
of this function. Also a little different
- the output from the function was converted

72
00:07:16,320 --> 00:07:25,210
from a data frame to numbers using the "as.numeric"
function. This was done so that the outputs

73
00:07:25,210 --> 00:07:33,419
could be combine into a matrix that allows
for easier display of row and column names.

74
00:07:33,419 --> 00:07:38,860
You will see that R gives users quite a bit
of flexibility. There is more than one way

75
00:07:38,860 --> 00:07:41,370
to accomplish an end goal.

76
00:07:41,370 --> 00:07:44,099
OVERHEAD 7 - Cost of Insurance

77
00:07:44,099 --> 00:07:51,970
To wrap up this video on claims analytics,
I want to briefly mention some thoughts on

78
00:07:51,970 --> 00:07:58,580
how a claims distribution can be used to determine
the cost of insurance. I first note that the

79
00:07:58,580 --> 00:08:05,819
Wisconsin property fund is a pool of government
entities. Unlike commercial insurers, it does

80
00:08:05,819 --> 00:08:12,220
no underwriting and coverage cannot be denied.
This makes our job of assessing the cost of

81
00:08:12,220 --> 00:08:18,550
insurance a bit easier - we can treat experience
from all policyholders as representative of

82
00:08:18,550 --> 00:08:20,500
the pool of interest.

83
00:08:20,500 --> 00:08:25,730
Now, to determine the cost of insurance, if
we make a decision based only on 2010 data,

84
00:08:25,730 --> 00:08:31,020
we might use 33,026.
This number was determined by dividing the

85
00:08:31,020 --> 00:08:36,320
total payout, the total fund claims, by the
number of policyholders.

86
00:08:36,320 --> 00:08:44,800
However, we get a very different answer based
on 2009 data (it turns out to be 9,934). This

87
00:08:44,800 --> 00:08:49,290
can be attributed to the volatility of the
claims distributions.

88
00:08:49,290 --> 00:08:56,450
Moreover, having a single price for all policyholders
is nice but hardly seems fair. Should small

89
00:08:56,450 --> 00:09:02,880
government libraries be charged the same as
the city of Green Bay? In the next video,

90
00:09:02,880 --> 00:09:10,260
we introduce the basics of rating variables,
designed to put the costs among different

91
00:09:10,260 --> 00:09:13,260
entities on a more equitable footing.

92
00:09:13,260 --> 00:09:16,540
Thanks for watching!

