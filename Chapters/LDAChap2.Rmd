
# Frequency Modeling

**Chapter Description**

A primary focus for insurers is estimating the magnitude of aggregate claims it must bear under its insurance contracts. Aggregate claims are affected by both the frequency and the severity of the insured event. Decomposing aggregate claims into these two components, each of which warrant significant attention, is essential for analysis and pricing. This chapter discusses frequency distributions, summary measures, and parameter estimation techniques.

:::: {.blackbox }

-  Although not needed to go through the tutorials, some users may wish to download the overheads that the videos are based on. <button download><a href="https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/LDA1.Overheads/LDA1.Chap2.pdf">Download Chapter Two overheads as a .pdf file.</a></button>
-  By watching the videos and working through the tutorial exercises, you will get an appreciation for frequency modeling. For a deeper dive, see the corresponding chapter in the textbook, [Chapter Two of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html).
:::: 


## Basic Frequency Distributions 

***

In this section, you learn how to:

*  Determine quantities that summarize a distribution such as the (cumulative) distribution as well as moments such as the mean and variance.
*  Define and compute the moment and probability generating functions.
*  Describe and understand relationships among three important frequency distributions, the binomial, Poisson, and negative binomial distributions.

***

####  Video: Basic Frequency Distributions {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_f9vceoba&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_m12oeksz" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Basic Frequency Distributions (Click Tab to View) {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect21A')">A. Frequency Distributions</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21B')">B. Foundations</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21C')">C. Probability Generating Function</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21D')">D. Important Frequency Distributions</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21E')">E. Poisson Distribution</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21F')">F. Binomial Distribution</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21G')">G. Negative Binomial Distribution</button>
  <button class="tablinks" onclick="openTab(event, 'Sect21H')">H. Review</button>
  </div>

<div id="Sect21A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=10" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=11" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=12" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=13" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=14" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=15" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21G" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=16" width="100%" height="400"> </iframe>
  </div>
<div id="Sect21H" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=17" width="100%" height="400"> </iframe>
  </div>



### Exercise. Representing the Number of Cyber Events with a Binomial Distribution


**Assignment Text**

Cyber risk for a firm is based on its liability for a data breach involving sensitive customer information, such as Social Security numbers, credit card numbers, account numbers, driver's license numbers and health records. A company models its cyber risk using the following assumptions:

(i) In any calendar quarter, there can be at most one cyber event.
(ii) In any calendar quarter, the probability of a cyber event is 0.1.
(iii) The numbers of cyber events in different calendar quarters are mutually independent.

Based on these assumptions, you represent the total number of cyber events as a binomial distribution.


:::: {.blackbox }
**Instructions** 

-  Identify the binomial distribution parameters for the number of cyber events in a 12 quarter (3 year) period.
-  Calculate the probability that there are $k$ cyber events for $k = 0, 1, \ldots, 12$ using the function [dbinom()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Binomial).
-  Create a data frame to present your results. All values within a specific column should be rounded to the same number of decimal places. Display the data frame.
-  Graph the probability mass function of the number of cyber events using the function [barplot()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/barplot). Include a descriptive title and axis labels for the graph.
::::

<br>



```{r ex="LDA1.2.1.1", type="pre-exercise-code", tut=TRUE}
# None for this exercise
```

```{r ex="LDA1.2.1.1", type="hint", tut=TRUE}
Review the important count distributions in [Loss Data Analytics Frequency Modeling](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:important-frequency-distributions) chapter.

```


```{r ex="LDA1.2.1.1", type="sample-code", tut=TRUE}
size = ??
Cyber.prob = ??

outcomes <- 0:size
pmf <- dbinom(x=outcomes, size=size, prob=Cyber.prob)  
pmf

pmf1 <- round(pmf, digits = 6)
outcomedataf <- rbind(outcomes, ??)
outcomedataf 

barplot(pmf, names.arg=??, col="lightgreen", ylab = ??, xlab = ??)
 
```


```{r ex="LDA1.2.1.1", type="solution", tut=TRUE}

size = 12
Cyber.prob = 0.01

outcomes <- 0:size
pmf <- dbinom(x=outcomes, size=size, prob=Cyber.prob)  
pmf

pmf1 <- round(pmf, digits = 6)
outcomedataf <- rbind(outcomes, pmf1)
outcomedataf 

barplot(pmf, names.arg=outcomes, col="lightgreen", ylab = "Probability", xlab = "Cyber Events")
```

```{r eval = FALSE, echo = FALSE}
> CheckObj <- function(term){
+ nameterm <- names(term)    
+ ex() %>% check_object("term", undefined_msg = "Make sure to not remove `nameterm`!") %>% 
+   check_equal(incorrect_msg="Did you correctly specify the object `nameterm`?")  
+ #paste(A1,term,A2,Glossfunction(term),A3, sep="")
+ }
> 
```

```{r ex="LDA1.2.1.1", type="sct", tut=TRUE}
sizemsg <- "Did you correctly specify the object `size`?"
ex() %>% check_object("size", undefined_msg = "Make sure to not remove `size`!") %>% check_equal(incorrect_msg=sizemsg)
Cyber.probmsg <- "Did you correctly specify the object `Cyber.prob`?"
ex() %>% check_object("Cyber.prob", undefined_msg = "Make sure to not remove `Cyber.prob`!") %>% check_equal(incorrect_msg=Cyber.probmsg)
outcomedatafmsg <- "Did you correctly specify the object `outcomedataf`?"
ex() %>% check_object("outcomedataf", undefined_msg = "Make sure to not remove `outcomedataf`!") %>% check_equal(incorrect_msg=outcomedatafmsg)
barplotmsg <- "Did you correctly specify the arguments?"
ex() %>% check_function("barplot", not_called_msg =barplotmsg) %>% check_result(error_msg=barplotmsg) %>% check_equal()

success_msg("Excellent job! Most consumers have some familiarity with the binomial distributions, so it is important in actuarial applications to be able to convey messages about counts in the context of this distribution.")
```

### Exercise.  Representing the Number of Cyber Events with a Poisson Distribution

**Assignment Text**

Another company is also concerned with cyber risk. Compared to the company in the prior exercise, this company is larger and does not wish to assume at most one cyber event in a quarter. Moreover, it believes that the distribution of cyber events is a function of its technical support staff size that has increased over time. Thus, it wishes to model the number of cyber events as a Poisson distribution with expected number of events as:

$$
{\small
\begin{array}{l|cccccc} \hline
\text{Quarter} & 1 & 2 & 3& 4& 5& 6 \\\hline
\text{Expected Number}& 0.1 & 0.1 & 0.1 & 0.1 & 0.2 & 0.2 \\ \hline
\text{Quarter} & 7 & 8 & 9& 10& 11& 12 \\ \hline 
\text{Expected Number}& 0.2 & 0.2 & 0.3 & 0.3 & 0.4 & 0.5 \\ \hline
\end{array}
}
$$

Assuming that the numbers of cyber events in different calendar quarters are mutually independent, the total number of cyber events over the three year period (12 quarters) has a Poisson distribution with expected number $\lambda = 2.7$. (Recall that the sum of independent Poisson random variables has a Poisson distribution.)

:::: {.blackbox }
**Instructions** 

-  Graph the probability mass function (pmf) of the number of cyber events using the function `barplot()`. 
-  Calculate the *pmf* and the cumulative probability distribution function for $k = 0, 1, \ldots, 12$ cyber events
using the functions [dpois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson), [ppois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson). Create a data frame to present your results and display the data frame.
-  From your data frame, identify the 95th percentile. Confirm your result using the [qpois()](https://www.rdocumentation.org/packages/stats/versions/3.3/topics/Poisson) function. 
-  How are the probabilities changing over time? Plot the probability of zero cyber events versus quarter number $k = 0, 1, \ldots, 12$.
::::

<br>


```{r ex="LDA1.2.1.2", type="hint", tut=TRUE}
You will find more discussion of percentiles in Section 4.1 of [Loss Data Analytics](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#S:MS:NonParInf).

```


```{r ex="LDA1.2.1.2", type="sample-code", tut=TRUE}
lambdavec <- c(rep(0.1,4), rep(0.2,4), 0.3, 0.3, 0.4, 0.5)
Cyber.lambda <- sum(??)

outcomes <- 0:12
Cyber.pmf <- dpois(x=??, lambda=Cyber.lambda)
barplot(Cyber.pmf, names.arg=outcomes, col="lightgreen", ylab = "Probability", xlab = "Cyber Events")

Cyber.df <- ppois(outcomes, ??)
outcomedataf <- rbind(outcomes, Cyber.pmf, Cyber.df)
outcomedataf 

qpois(??, lambda=Cyber.lambda)

Probzero <- dpois(x=0, lambda=lambdavec)
plot(??,Probzero, type = "l", xlab = "quarter", ylab = "Prob of Zero")

```


```{r ex="LDA1.2.1.2", type="solution", tut=TRUE}

lambdavec <- c(rep(0.1,4), rep(0.2,4), 0.3, 0.3, 0.4, 0.5)
Cyber.lambda <- sum(lambdavec)

outcomes <- 0:12
Cyber.pmf <- dpois(x=outcomes, lambda=Cyber.lambda)
barplot(Cyber.pmf, names.arg=outcomes, col="lightgreen", ylab = "Probability", xlab = "Cyber Events")

Cyber.df <- ppois(outcomes, lambda=Cyber.lambda)
outcomedataf <- rbind(outcomes, Cyber.pmf, Cyber.df)
outcomedataf 

qpois(0.95, lambda=Cyber.lambda)

Probzero <- dpois(x=0, lambda=lambdavec)
plot(1:12,Probzero, type = "l", xlab = "quarter", ylab = "Prob of Zero")
```

```{r ex="LDA1.2.1.2", type="sct", tut=TRUE}
Cyber.lambdamsg <- "Did you correctly specify the object `Cyber.lambda`?"
ex() %>% check_object("Cyber.lambda", undefined_msg = "Make sure to not remove `Cyber.lambda`!") %>% check_equal(incorrect_msg=Cyber.lambdamsg)
Cyber.pmfmsg <- "Did you correctly specify the object `Cyber.pmf`?"
ex() %>% check_object("Cyber.pmf", undefined_msg = "Make sure to not remove `Cyber.pmf`!") %>% check_equal(incorrect_msg=Cyber.pmfmsg)
Cyber.dfmsg <- "Did you correctly specify the object `Cyber.df`?"
ex() %>% check_object("Cyber.df", undefined_msg = "Make sure to not remove `Cyber.df`!") %>% check_equal(incorrect_msg=Cyber.dfmsg)

success_msg("Excellent job! The Poisson is the basic count distribution in actuarial applications. Consumers often have familiarity with the expected number of events as well as the probability of zero events. In the last part, you learned how to portray increasing means in terms of decreasing frequency of zero events, connecting these important summary measures.")
```

### Exercise. Comparing Basic Count Distributions

**Assignment Text**

Your supervisor would like to have a better understanding of relationships among three important count distributions, the binomial, Poisson, and negative binomial. You could develop a mathematical appendix, demonstrating how:

*  A binomial distribution with parameters $m \to \infty$ and $mq \to \lambda$ converges to a Poisson distribution.
*  A negative binomial distribution with mean parameter $r \beta = \lambda$ and dispersion parameter $r$ converges to a Poisson distribution as $r \to \infty$.

Instead, you decide to demonstrate these relationships graphically.

:::: {.blackbox }
**Instructions** 

-  Plot the probability mass function (pmf) of the binomial distribution with $m=12$ and $q=0.1$ over $k = 0, 1, \ldots, 12$ potential outcomes. Superimpose on this plot a Poisson *pmf* with the same mean using the [lines()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/lines) function.
-  Repeat this step with the same Poisson distribution but, for the binomial distribution, multiply $m$ by 5 and divide $q$ by 5. (You should see how the binomial becomes a better approximation to the Poisson.) 
-  Determine the *pmf* of the negative binomial distribution with mean parameter $r \beta$ and dispersion parameter $r=1$ using the function [dnbinom()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/NegBinomial). Use the same mean as for the binomial distribution.
-  Demonstrate the convergence of the negative binomial to the Poisson by creating side-by-side graphical comparisons. That is, using the [par(mfrow = ...)](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/par) syntax, compare:
    -  A plot of this negative binomial distribution pmf, superimposed with baseline Poisson distribution (with the same mean). 
    -  A plot of the negative binomial distribution *pmf* with the same mean and dispersion parameter $r=100$, superimposed with baseline Poisson distribution.
::::

<br>

```{r ex="LDA1.2.1.3", type="hint", tut=TRUE}
To review the negative binomial distribution, see [Discrete Distributions](https://ewfrees.github.io/Loss-Data-Analytics/C-SummaryDistributions.html#S:DiscreteDistributions)

```


```{r ex="LDA1.2.1.3", type="sample-code", tut=TRUE}
outcomes <- 0:12

Binom1.pmf  <- dbinom(x=outcomes, size=12, ??)  
Poisson.pmf <- dpois(x=outcomes, lambda=1.2)
plot(outcomes, ??)
lines(outcomes, ??)

Binom2.pmf <- dbinom(x=outcomes, size=??, prob=??) 
plot(outcomes, Binom2.pmf)
lines(outcomes, Poisson.pmf)

( NegBinom1.pmf <- dnbinom(outcomes, mu=?? size=??) )

par(mfrow = c(1,2))
plot(outcomes, NegBinom1.pmf)
lines(outcomes, Poisson.pmf)
NegBinom2.pmf <- dnbinom(outcomes, mu=??, size=??)   
plot(outcomes, NegBinom2.pmf)
lines(outcomes, Poisson.pmf)
```


```{r ex="LDA1.2.1.3", type="solution", tut=TRUE}
outcomes <- 0:12

Binom1.pmf  <- dbinom(x=outcomes, size=12, prob=0.1)  
Poisson.pmf <- dpois(x=outcomes, lambda=1.2)
plot(outcomes, Binom1.pmf)
lines(outcomes, Poisson.pmf)

Binom2.pmf <- dbinom(x=outcomes, size=12*5, prob=0.1/5) 
plot(outcomes, Binom2.pmf)
lines(outcomes, Poisson.pmf)

( NegBinom1.pmf <- dnbinom(outcomes, mu=1.2, size=1) )

par(mfrow = c(1,2))
plot(outcomes, NegBinom1.pmf)
lines(outcomes, Poisson.pmf)
NegBinom2.pmf <- dnbinom(outcomes, mu=1.2, size=100)   
plot(outcomes, NegBinom2.pmf)
lines(outcomes, Poisson.pmf)

```

```{r ex="LDA1.2.1.3", type="sct", tut=TRUE}
Binom1.pmfmsg <- "Did you correctly specify the object `Binom1.pmf`?"
ex() %>% check_object("Binom1.pmf", undefined_msg = "Make sure to not remove `Binom1.pmf`!") %>% check_equal(incorrect_msg=Binom1.pmfmsg)
Binom2.pmfmsg <- "Did you correctly specify the object `Binom2.pmf`?"
ex() %>% check_object("Binom2.pmf", undefined_msg = "Make sure to not remove `Binom2.pmf`!") %>% check_equal(incorrect_msg=Binom2.pmfmsg)
NegBinom1.pmfmsg <- "Did you correctly specify the object `NegBinom1.pmf`?"
ex() %>% check_object("NegBinom1.pmf", undefined_msg = "Make sure to not remove `NegBinom1.pmf`!") %>% check_equal(incorrect_msg=NegBinom1.pmfmsg)
NegBinom2.pmfmsg <- "Did you correctly specify the object `NegBinom2.pmf`?"
ex() %>% check_object("NegBinom2.pmf", undefined_msg = "Make sure to not remove `NegBinom2.pmf`!") %>% check_equal(incorrect_msg=NegBinom2.pmfmsg)

success_msg("Excellent job! As you study actuarial data applications, do not shy away from the mathematics! Often, the discipline uses the rigor of mathematics to crystallize important ideas. One of your jobs is to be able to communicate these ideas to a broader public. You will find that graphical presentations are helpful in this regard.")
```


## The (a,b,0) Class

***

In this section, you learn how to:

*    Define the $(a,b,0)$ class of frequency distributions.
*    Discuss the importance of the recursive relationship underpinning this class of distributions.
*    Identify conditions under which this general class reduces to each of the binomial, Poisson, and negative binomial distributions.

***

####  Video: The (a,b,0) Class {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_wmfy8z1y&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_qhfqhk7r" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: The (a,b,0) Class (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect22A')">A. (a, b, 0) Class</button>
  <button class="tablinks" onclick="openTab(event, 'Sect22B')">B. (a, b, 0) Class - Special Cases</button>
  <button class="tablinks" onclick="openTab(event, 'Sect22C')">C. Review</button>
  </div>

<div id="Sect22A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=19" width="100%" height="400"> </iframe>
  </div>
<div id="Sect22B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=20" width="100%" height="400"> </iframe>
  </div>
<div id="Sect22C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=21" width="100%" height="400"> </iframe>
  </div>



### Exercise. Determining Probabilities Recursively

**Assignment Text**

The $(a,b,0)$ class can be expressed through the recursion

$$
\Pr(N=k) = p_k = p_{k-1} \left( a+ \frac{b}{k}\right) , \quad k\ge 1 ,
$$

where $N$ is a count random variable. From Section 2.3 of the text, we know that:

*  if $a=0$ and $b=\lambda$, then the recursion yields a Poisson distribution with parameter $\lambda$
*  if $a=-q/(1-q)$ and $b=(m+1)q/(1+q)$, then the recursion yields a binomial distribution with parameters $m$ and $q$
*  if $a=\beta/(1+\beta)$ and $b=(r-1)\beta/(1+\beta)$, then the recursion yields a negative binomial distribution with parameters $r$ and $\beta$.

The $(a,b,0)$ class is a foundation for other, more complex, distributions, so let us check that we understand the recursions.


:::: {.blackbox }
**Instructions**

-  For $k=0, \ldots, 20$, using $\lambda = 1.24$ obtain $p_k$ values using  [dpois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson).
-  With the starting value $p_0 = \exp(-\lambda)$, use the recursive $(a,b,0)$ formula to obtain these probability values.
-  Check your code by summing over the absolute value of the differences between the `dpois` and the $(a,b,0)$ generated values.
-   For $k=0, \ldots, 20$,  obtain $p_k$ values using the negative binomial distribution using the function [dnbinom()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/NegBinomial). Use the same mean as for the Poisson distribution but let the variance be 1.1 times the mean. *Hint*. See the [Loss Data Analytics Summary of Distributions](https://openacttexts.github.io/Loss-Data-Analytics/C-SummaryDistributions.html#discrete-distributions) for the parameterization used in this short course. It differs from that used by the `R` package.
-  Use the recursive $(a,b,0)$ formula to obtain these probability values. 
-  Check your code by summing over the absolute value of the differences between the `dnbinom` and the $(a,b,0)$ generated values.
::::

<br>


```{r ex="LDA1.2.2.1", type="hint", tut=TRUE}
 There are several ways to do recursions in `R`, you might start with a simple [for()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/Control) expression.

```


```{r ex="LDA1.2.2.1", type="pre-exercise-code", tut=TRUE}
# None Needed for this Exercise
```



```{r ex="LDA1.2.2.1", type="sample-code", tut=TRUE}

kvec <- c(0:??) 
Poisson.p_k <- dpois(x=kvec, lambda=??) 
a = ?? 
b = ??
ab0.vecP <-  exp(-1.24) -> p.new
for (k in 2:21){
  p.new <- p.new *(a+b/(k-1))
  ab0.vecP <- append(ab0.vecP,p.new)
  }
sum(abs(Poisson.p_k - ??))

NegBinom.p_k <- dnbinom(kvec, prob=1/1.1, size = 12.4)
a = ?? 
b = ??
ab0.vecNB <-  1.1**(-12.4) -> p.new
for (k in ??){
  p.new <- p.new *(a+b/k)
  ab0.vecNB <- append(ab0.vecNB,p.new)
  }
sum(abs(NegBinom.p_k - ??))

```



```{r ex="LDA1.2.2.1", type="solution", tut=TRUE}
kvec <- c(0:20) 
Poisson.p_k <- dpois(x=kvec, lambda=1.24) 
a = 0; 
b = 1.24
ab0.vecP <-  exp(-1.24) -> p.new
for (k in 1:20){
  p.new <- p.new *(a+b/k)
  ab0.vecP <- append(ab0.vecP,p.new)
  }
sum(abs(Poisson.p_k - ab0.vecP))

NegBinom.p_k <- dnbinom(kvec, prob=1/1.1, size = 12.4)
a = .1/1.1; 
b = 11.4*(0.1)/1.1
ab0.vecNB <-  1.1**(-12.4) -> p.new
for (k in 2:21){
  p.new <- p.new *(a+b/(k-1))
  ab0.vecNB <- append(ab0.vecNB,p.new)
  }
sum(abs(NegBinom.p_k - ab0.vecNB))
```

```{r ex="LDA1.2.2.1", type="sct", tut=TRUE}
kvecmsg <- "Did you correctly specify the object `kvec`?"
ex() %>% check_object("kvec", undefined_msg = "Make sure to not remove `kvec`!") %>% check_equal(incorrect_msg=kvecmsg)
Poisson.p_kmsg <- "Did you correctly specify the object `Poisson.p_k`?"
ex() %>% check_object("Poisson.p_k", undefined_msg = "Make sure to not remove `Poisson.p_k`!") %>% check_equal(incorrect_msg=Poisson.p_kmsg)
ab0.vecPmsg <- "Did you correctly specify the object `ab0.vecP`?"
ex() %>% check_object("ab0.vecP", undefined_msg = "Make sure to not remove `ab0.vecP`!") %>% check_equal(incorrect_msg=ab0.vecPmsg)
NegBinom.p_kmsg <- "Did you correctly specify the object `NegBinom.p_k`?"
ex() %>% check_object("NegBinom.p_k", undefined_msg = "Make sure to not remove `NegBinom.p_k`!") %>% check_equal(incorrect_msg=NegBinom.p_kmsg)
ab0.vecNBmsg <- "Did you correctly specify the object `ab0.vecNB`?"
ex() %>% check_object("ab0.vecNB", undefined_msg = "Make sure to not remove `ab0.vecNB`!") %>% check_equal(incorrect_msg=ab0.vecNBmsg)

success_msg("Excellent job! Being able to do calculations recursively, such as using for loops, is an important for actuarial models.")
```

### Exercise. Reverse Engineering and Recursive `R` Functions

**Assignment Text**

You want to generate probabilities from the $(a,b,0)$ class so that later on you will be able to modify your code to produce alternative distributions (the subject of Section 2.5), a bit of so-called "reverse engineering." In the first part of this problem, from a known distribution (e.g., the binomial), you will compute the ratio

$$
\frac{k ~p_k}{p_{k-1}} =  a k + b , \quad k\ge 1 ,
$$

to determine values of $a$ and $b$. The second part of this problem utilizes recursive `R` functions. This is a function defined in terms of the same function but at a prior iteration. The classic example is the factorial function $f(n) = n!$ so $f(n) = n f(n-1)$. For example, you can define the function

```
recursive.factorial <- function(n) {
  if (n == 0)    return (1)
  else           return (n * recursive.factorial(n-1))
  }
```
to determine that `recursive.factorial(5) = 120`. In this problem, we use a recursive `R` function to generate $(a,b,0)$ probabilities.


:::: {.blackbox }
**Instructions**

-  For $k=0, \ldots, 4$, using $\lambda = 1.24$ obtain $p_k$ values from the  [dpois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson) function.
-  Compute the ratio to identify values of $a$ and $b$. *Hint*: In `R`, use of *negative indexing* is permitted. For example, `pop[-c(3, 7)]` removes the third and seventh elements of `pop`.
-  For $k=0, \ldots, 4$, using $q =0.1$ and $m=10$ obtain $p_k$ values from the   [dbinom()](https://www.rdocumentation.org/packages/stats/versions/3.3/topics/Binomial) function.
-  Compute the ratio to identify values of $a$ and $b$.
-  To check your work, develop the recursive $(a,b,0)$ function to determine $p_4 = \Pr(N=4)$ based on the binomial distribution in the prior part.
::::

<br>


```{r ex="LDA1.2.2.2", type="hint", tut=TRUE}
You might wish to learn more about [R recursive functions](https://www.datamentor.io/r-programming/recursion/)

```


```{r ex="LDA1.2.2.2", type="pre-exercise-code", tut=TRUE}
# None Needed for this Exercise
```



```{r ex="LDA1.2.2.2", type="sample-code", tut=TRUE}
kvec <- c(0:4)  
Poisson.p_k <- dpois(x=kvec, lambda=1.24) 
kvec[??] * Poisson.p_k[??/Poisson.p_k[-length(kvec)]

Binom.p_k <- dbinom(x=kvec, prob = 0.1, size = 10) 
outvec <- kvec[??] * Binom.p_k[??]/Binom.p_k[-length(kvec)]
( a = ?? - ?? )
( b = ?? - a )

# Recursive function to find factorial
recursive.ab0 <- function(k) {
  if (k == 0)  return ( Binom.p_k[1] )
  else         return ( (a+b/k) * recursive.ab0(k-1) )
  }
recursive.ab0(??)
Binom.p_k[??]

```



```{r ex="LDA1.2.2.2", type="solution", tut=TRUE}
kvec <- c(0:4)  
Poisson.p_k <- dpois(x=kvec, lambda=1.24) 
kvec[-1] * Poisson.p_k[-1]/Poisson.p_k[-length(kvec)]

Binom.p_k <- dbinom(x=kvec, prob = 0.1, size = 10) 
outvec <- kvec[-1] * Binom.p_k[-1]/Binom.p_k[-length(kvec)]
( a = outvec[2] - outvec[1] )
( b = outvec[1] - a )

# Recursive function to find factorial
recursive.ab0 <- function(k) {
  if (k == 0)  return ( Binom.p_k[1] )
  else         return ( (a+b/k) * recursive.ab0(k-1) )
  }
recursive.ab0(4)
Binom.p_k[5]
```

```{r ex="LDA1.2.2.2", type="sct", tut=TRUE}
Poisson.p_kmsg <- "Did you correctly specify the object `Poisson.p_k`?"
ex() %>% check_object("Poisson.p_k", undefined_msg = "Make sure to not remove `Poisson.p_k`!") %>%check_equal(incorrect_msg=Poisson.p_kmsg)
Binom.p_kmsg <- "Did you correctly specify the object `Binom.p_k`?"
ex() %>% check_object("Binom.p_k", undefined_msg = "Make sure to not remove `Binom.p_k`!") %>%check_equal(incorrect_msg=Binom.p_kmsg)
recursive.ab0msg <- "Did you correctly specify the object `recursive.ab0`?"
ex() %>% check_object("recursive.ab0", undefined_msg = "Make sure to not remove `recursive.ab0`!") %>%check_equal(incorrect_msg=recursive.ab0msg)

success_msg("Superb! Working forwards and backwards is important in complex tasks. The idea of 'reverse engineering' is common in complex business settings. ")
```


## Estimating Frequency Distributions

***

In this section, you learn how to:

*    Define a likelihood for a sample of observations from a discrete distribution.
*    Define the maximum likelihood estimator (*mle*) for a random sample of observations from a discrete distribution.
*    Calculate the *mle* for the binomial, Poisson, and negative binomial distributions.

***


####  Video: Estimating Frequency Distributions {-}

<center>


<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_wr2ht513&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_78m5tjmh" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>


#### Overheads: Estimating Frequency Distributions  (Click Tab to View) {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect231A')">A. Basic Problem</button>
  <button class="tablinks" onclick="openTab(event, 'Sect231B')">B. Compression of Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect231C')">C. The Likelihood</button>
  <button class="tablinks" onclick="openTab(event, 'Sect231D')">D. Maximum Likelihood (ML) Estimation</button>
  <button class="tablinks" onclick="openTab(event, 'Sect231E')">E. ML Estimation - General Setup</button>
  <button class="tablinks" onclick="openTab(event, 'Sect231F')">F. Plot of Likelihood and Log-likelihood</button>
  </div>

<div id="Sect231A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=23" width="100%" height="400"> </iframe>
  </div>
<div id="Sect231B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=24" width="100%" height="400"> </iframe>
  </div>
<div id="Sect231C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=25" width="100%" height="400"> </iframe>
  </div>
<div id="Sect231D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=26" width="100%" height="400"> </iframe>
  </div>
<div id="Sect231E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=27" width="100%" height="400"> </iframe>
  </div>
<div id="Sect231F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=28" width="100%" height="400"> </iframe>
  </div>


### Exercise. Count Data Compression - 1 {#Ex:CountDataCompression}

**Assignment Text**

Raw count data can often be compressed without loss of any statistical information. Typically, the compression is to the sequence of values $\{m_k\}_{k\geq 0}$, where $m_k$ is the number of observations equal to $k$, that is, $m_k=\sum_{i= 1}^n I(x_i=k).$ In this and the following exercise, we discuss two data structures for this compressed data and suggest implementations in `R`.

:::: {.blackbox }
**Instructions**

- Store the count data $\{3, 6, 0, 2, 3, 4, 4, 2, 4, 4, 6, 2, 3, 0, 2, 3, 1, 2, 4, 2\}$ into an array, say $\bf x$, using the [c()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/c) (for concatenate) function 
- Use the [table()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/table) function to generate a frequency table of the data.
- Initialize an array (say $\bf m.vec$) to hold the summarized counts of frequency, $m_0, m_1, \ldots$. Do this using the  [rep()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/rep) function that replicates the values in $x$. (It is a generic function, and the (internal) default method is described here.)
-  Fill in the appropriate values into the array $\bf m.vec$. Some illustrative code uses the functions [names()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/names) (functions to get or set the names of an object),  [as.integer()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/integer) (it creates or tests for objects of type "integer"), and 
[as.vector()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/vector) (it produces a vector of the given length and mode).
-  Use the array $\bf m.vec$ to determine the mean frequency.
-  With the array $\bf m.vec$, determine the count distribution using the function [barplot()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/barplot) (it creates a bar plot with vertical or horizontal bars).
:::: 

<br>

```{r ex="LDA1.2.3.1", type="hint", tut=TRUE}
Play with a smaller data set (i.e. x) to understand each line of code.

```


```{r ex="LDA1.2.3.1", type="sample-code", tut=TRUE}
x <- c(??)

table(??)

m.vec <- rep(0,max(??)+1)
# Fill in appropriately
??[as.integer(names(table(??)))+1] = as.vector(table(??))

sum((0:(length(??)-1))*??)/sum(??)

barplot(??,names.arg=0:(length(??)-1),ylab="Frequency", xlab="Count")

```

```{r ex="LDA1.2.3.1", type="solution", tut=TRUE}
x <- c(3, 6, 0, 2, 3, 4, 4, 2, 4, 4, 6, 2, 3, 0, 2, 3,
     1, 2, 4, 2)

table(x)

m.vec <- rep(0,max(x)+1)
# Fill in appropriately
m.vec[as.integer(names(table(x)))+1]=as.vector(table(x))

sum((0:(length(m.vec)-1))*m.vec)/sum(m.vec)

barplot(m.vec,names.arg=0:(length(m.vec)-1),ylab="Frequency",xlab="Count")

```

```{r ex="LDA1.2.3.1", type="sct", tut=TRUE}
xmsg <- "Did you correctly specify the object `x`?"
ex() %>% check_object("x", undefined_msg = "Make sure to not remove `x`!") %>%check_equal(incorrect_msg=xmsg)
m.vecmsg <- "Did you correctly specify the object `m.vec`?"
ex() %>% check_object("m.vec", undefined_msg = "Make sure to not remove `m.vec`!") %>%check_equal(incorrect_msg=m.vecmsg)
tablemsg <- "Did you correctly specify the arguments?"
ex() %>% check_function("table", not_called_msg =tablemsg) %>% check_result(error_msg=tablemsg) %>% check_equal()
barplotmsg <- "Did you correctly specify the arguments?"
ex() %>% check_function("barplot", not_called_msg =barplotmsg) %>% check_result(error_msg=barplotmsg) %>% check_equal()
success_msg("Excellent job! Actuarial applications often involve massive datasets. It can be handy identifying situations in which data can be compressed without loss of information. Some applications of count data is one such situation.")
```



### Exercise. Count Data Compression - 2

**Assignment Text**

In Exercise \@ref(Ex:CountDataCompression), the count data range was narrow - counts range from 0 to 6. In that case the suggested manner to store $m_k$'s worked well. In Section \@ref(Sec:LGPIF) of this short course, the Wisconsin Property Fund data has been introduced which consists of claim experience for fund members over the years 2006-2010, inclusive. It includes the frequency of claims `Freq` as well as the claim year `Year`. The Wisconsin Property Fund data has already been read into a data frame called `Insample`. In this assignment, we will compress the claims frequency data using a data structure that differs from that presented in the preceding exercise. 


:::: {.blackbox }
**Instructions** 

-  Using the dataframe `Insample`, create a smaller data set based on year 2007 experience.
-  From the 2007 experience, create a frequency table.
-  Store distinct claim counts observed into an array named $\bf values$.
-  Store the frequency of claim counts into an array named $\bf m.vec$. 
-  Use the array $\bf m.vec$ to determine the mean frequency.
-  The frequency table shows one observation with 157 claims. Use the  [match()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/match) function to verify this. (This function returns a vector of the positions of (first) matches of its first argument in its second).
-  With the array $\bf m.vec$, graph the count distribution.
::::

<br>

```{r ex="LDA1.2.3.2", type="hint", tut=TRUE}
Play with a smaller data set (i.e. x) to understand each line of code.
```


```{r ex="LDA1.2.3.2", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/Data/Insample.csv", header=T,na.strings=c("."),stringsAsFactors=FALSE)
```


```{r ex="LDA1.2.3.2", type="sample-code", tut=TRUE}

Insampleyr <- subset(??, Year==??)

table(??$Freq)

values <- as.integer(names(table(??)))

m.vec <- as.vector(table(??))

sum(??*m.vec)/sum(??)
# Number of claim counts equal to 157
m.vec[match(??,??)]

barplot(??,names.arg=??,ylab="Frequency",xlab="Count")
```

```{r ex="LDA1.2.3.2", type="solution", tut=TRUE}

Insampleyr <- subset(Insample, Year==2007)

table(Insampleyr$Freq)

values <- as.integer(names(table(Insampleyr$Freq)))

m.vec <- as.vector(table(Insampleyr$Freq))

sum(values*m.vec)/sum(m.vec)
# Number of claim counts equal to 157
m.vec[match(157,values)]

barplot(m.vec,names.arg=values,ylab="Frequency",xlab="Count")
```

```{r ex="LDA1.2.3.2", type="sct", tut=TRUE}
Insampleyrmsg <- "Did you correctly specify the object `Insampleyr`?"
ex() %>% check_object("Insampleyr", undefined_msg = "Make sure to not remove `Insampleyr`!") %>%check_equal(incorrect_msg=Insampleyrmsg)
valuesmsg <- "Did you correctly specify the object `values`?"
ex() %>% check_object("values", undefined_msg = "Make sure to not remove `values`!") %>%check_equal(incorrect_msg=valuesmsg)
m.vecmsg <- "Did you correctly specify the object `m.vec`?"
ex() %>% check_object("m.vec", undefined_msg = "Make sure to not remove `m.vec`!") %>%check_equal(incorrect_msg=m.vecmsg)
tablemsg <- "Did you correctly specify the arguments?"
ex() %>% check_function("table", not_called_msg =tablemsg) %>% check_result(error_msg=tablemsg) %>% check_equal()
barplotmsg <- "Did you correctly specify the arguments?"
ex() %>% check_function("barplot", not_called_msg =barplotmsg) %>% check_result(error_msg=barplotmsg) %>% check_equal()
success_msg("Excellent job! There are 1138 distinct observations in the 2007 experience. Without any loss of information, these could be summarized into only 20 values, a terrific real world example of compression.")
```





####  Video: Fitting a Poisson Distribution {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_xmnkpby4&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_dqkx5yeu" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>


### Exercise. Graphing Likelihoods

**Assignment Text**

In this assignment you are asked to plot the Poisson likelihood and the log-likelihood for the data from Exercise \@ref(Ex:CountDataCompression). It is instructive to see that the maximum for both the curves are attained at the same point, the sample mean. These data have already been read into an array $\bf x$ with the counts in the array $\bf m.vec$. Once you have a working code, scroll through the plots to understand what each piece of graph code does. 

:::: {.blackbox }
**Instructions**

-  Write the likelihood as a function of the parameter $\theta$.
-  Create an array with values of $\theta = 1, 1.01, 1.02, \ldots, 5$.
-  Plot the likelihood over values of $\theta$. You may find useful the function [sapply()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sapply). (It applies a function over a list or vector.)
-  Plot the log-likelihood over values of $\theta$.
-  Superimpose a vertical line at the mean to emphasize that both the likelihood and the log-likelihood reach their maximum values at the mean. For this, use the [abline()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/abline) (this function adds one or more straight lines through the current plot).
::::

<br>

```{r ex="LDA1.2.3.3", type="hint", tut=TRUE}
To simplify matters even more, you could try with just three points.
```

```{r ex="LDA1.2.3.3", type="pre-exercise-code", tut=TRUE}
x<-c(3, 6, 0, 2, 3, 4, 4, 2, 4, 4, 6, 2, 3, 0, 2, 3,
     1, 2, 4, 2)
m.vec <- rep(0,max(x)+1)
m.vec[as.integer(names(table(x)))+1]=as.vector(table(x))
```


```{r ex="LDA1.2.3.3", type="sample-code", tut=TRUE}
# Likelihood Function
Like  <-  function(theta){
  prod(dpois(0:max(x),theta)^??)
  }
theta <- ??
  
par(mar=c(5, 5, 4, 5) + 0.1) # Plot Margins
# Plot Likelihood
plot(theta,sapply(??,Like),type="l",axes=FALSE,xlab="",ylab="")
axis(2, ylim=c(0,5e-17),col="black",las=1) # First y-axis
mtext("Likelihood",side=2,line=3.75)

par(new=TRUE) # Reuse Graph
# Plot Log-likelihood
plot(??,log(sapply(theta,??)),axes=FALSE,xlab="",ylab="",ylim=c(-65,-35),col="red",type="l")
mtext("Log-likelihood",side=4,col="red",line=3)
axis(4, ylim=c(-25,-16), col="red",col.axis="red", las=1) # Second y-axis
axis(1, c(1,1.5,2,2.5,mean(x),3.5,4,4.5,5) ) # Common x-axis
mtext(expression(theta),side=1,col="black",line=2.5)
# Vertical line at the common argmax (mle)
abline(v=mean(??),col="green")
```

```{r ex="LDA1.2.3.3", type="solution", tut=TRUE}
# Likelihood Function
Like <- function(theta){
  prod(dpois(0:max(x),theta)^m.vec)
  }
theta <- (100:500)/100

par(mar=c(5, 5, 4, 5) + 0.1) # Plot Margins
# Plot Likelihood
plot(theta,sapply(theta,Like),type="l",axes=FALSE,xlab="",ylab="")
axis(2, ylim=c(0,5e-17),col="black",las=1) # First y-axis
mtext("Likelihood",side=2,line=3.75)

par(new=TRUE) # Reuse Graph
# Plot Log-likelihood
plot(theta,log(sapply(theta,Like)),axes=FALSE,xlab="",ylab="",ylim=c(-65,-35),col="red",type="l")
mtext("Log-likelihood",side=4,col="red",line=3)
axis(4, ylim=c(-25,-16), col="red",col.axis="red",las=1) # Second y-axis
axis(1,c(1,1.5,2,2.5,mean(x),3.5,4,4.5,5)) # Common x-axis
mtext(expression(theta),side=1,col="black",line=2.5)
# Vertical line at the common argmax (mle)
abline(v=mean(x),col="green")
```

```{r ex="LDA1.2.3.3", type="sct", tut=TRUE}
thetamsg <- "Did you correctly specify the object `theta`?"
ex() %>% check_object("theta", undefined_msg = "Make sure to not remove `theta`!") %>%check_equal(incorrect_msg=thetamsg)
success_msg("Excellent job! In complex actuarial applications, maximum likelihood is used extensively to calibrate models by estimating the most likely parameter values. One gets insights into this process by visualizing functions that are being maximized.")
```


####  Video: Fitting Binominal and Negative Binomial Distributions {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_ys8n0r8y&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_z2zfn7ka" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>


#### Overheads: Fitting Binominal and Negative Binomial Distributions  (Click Tab to View)  {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect233A')">A. Binomial Model</button>
  <button class="tablinks" onclick="openTab(event, 'Sect233B')">B. Negative Binomial Model</button>
  <button class="tablinks" onclick="openTab(event, 'Sect233C')">C. Reason Behind non-existence of MLE</button>
  <button class="tablinks" onclick="openTab(event, 'Sect233D')">D. REVIEW</button>
  </div>

<div id="Sect233A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=30" width="100%" height="400"> </iframe>
  </div>
<div id="Sect233B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=31" width="100%" height="400"> </iframe>
  </div>
<div id="Sect233C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=32" width="100%" height="400"> </iframe>
  </div>
<div id="Sect233D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=33" width="100%" height="400"> </iframe>
  </div>


### Exercise. Fitting a Binomial Distribution

**Assignment Text**

In this assignment you are asked to fit the binomial model to a small set of count data, $\{1, 3, 3, 3, 5, 3, 0, 2, 4, 3, 4\}$. Recall that in this parameterization of the binomial model, $m$ is the potential number of 1's (the number of "trials").
You will recall from [Section 2.4.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:frequency-distributions-mle) that when $m$ is known, the *mle* of $q$ is simply a sample average. Because $m$ is restricted to integer values, it is convenient to resort to brute force maximization of the *reduced* likelihood in order to find the *mle* for $m$. But then it is a must to visually confirm the solution by plotting the reduced log-likelihood. 

Note that it is important to check that the sample mean exceeds sample variance for the *mle* of $m$ to be finite - lest the Poisson is the better model. The data are available in an array $\bf x$ and the array $\bf m.vec$ of Exercise \@ref(Ex:CountDataCompression) is also made available.

:::: {.blackbox }
**Instructions**

- Develop the reduced likelihood function based on the compressed data in the array $\bf m.vec$.
- The value of $m$ must be at least as large as the largest observed value in the sample. Calculate potential values of the reduced likelihood over a range beginning from the maximum value in $\bf m.vec$.
- Determine the maximum likelihood estimators of the parameters $m$ and $q$ by selecting the largest likelihood. For this, you will find helpful the function [match()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/match) (it returns a vector of the positions of (first) matches of its first argument in its second).
- To check your results, plot the likelihood function over a range of $m$. Superimpose a vertical line at the *mle*.
- Check to see whether the sample mean is greater than the sample variance. Doing so, you can use the function [var()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/var) (it computes the variance using a denominator of $n-1$). To avoid this, the sample code instead uses `var(c(x,mean(x)))` instead of `var(x)`. Ask yourself why?
::::

<br>

```{r ex="LDA1.2.3.4", type="hint", tut=TRUE}
When interpreting the code, this exercise uses the more basic likelihood, in contrast to the more common log-likelihood function. Further, for a fixed *m*, the *mle* of *q* is simply the average. So, we only need to do one dimensional optimization even though there are two parameters.
```


```{r ex="LDA1.2.3.4", type="pre-exercise-code", tut=TRUE}
set.seed("1234")
x <- rbinom(11,9,1/3)
# x <- c(1, 3, 3, 3, 5, 3, 0, 2, 4, 3, 4)
m.vec <- rep(0,max(x)+1)
m.vec[as.integer(names(table(x)))+1] = as.vector(table(x))
```


```{r ex="LDA1.2.3.4", type="sample-code", tut=TRUE}
# Reduced Likelihood Function 
reduced_like<-function(para_m){
  if (para_m>=max(x))
    prod(dbinom(0:max(x),para_m,mean(x)/ceiling(para_m))^m.vec)
  else    0
  }
# Compute the reduced likelihood across a range of values for m
obj <- sapply(max(x):(3*max(x)),??)

# mle for m and q
m_MLE <- match(max(obj),obj)+max(x)-1
q_MLE <- mean(x)/??
# Visually check the reduced log-likelihood
plot(max(x):(3*max(x)),log(??),type="l",xlab="m",ylab=expression(l(m,bar(x)/m)))
abline(v=m_MLE,col="green")

# Check if sample mean > sample variance
c(mean(??), var(c(x,mean(x))))
```

```{r ex="LDA1.2.3.4", type="solution", tut=TRUE}
# Likelihood Function 
reduced_like<-function(para_m){
  if (para_m>=max(x))
    prod(dbinom(0:max(x),para_m,mean(x)/ceiling(para_m))^m.vec)
  else      0
  }
#  Compute the reduced likelihood across a range of values for m
obj <- sapply(max(x):(3*max(x)),reduced_like)

# MLE for m and q
m_MLE <- match(max(obj),obj)+max(x)-1
q_MLE <- mean(x)/m_MLE
# Important to visually check the reduced log-likelihood
plot(max(x):(3*max(x)),log(obj),type="l",xlab="m",ylab=expression(l(m,bar(x)/m)))
abline(v=m_MLE,col="green")

# Check if sample mean > sample variance
c(mean(x), var(c(x,mean(x))))
```

```{r ex="LDA1.2.3.4", type="sct", tut=TRUE}
objmsg <- "Did you correctly specify the object `obj`?"
ex() %>% check_object("obj", undefined_msg = "Make sure to not remove `obj`!") %>%check_equal(incorrect_msg=objmsg)
m_MLEmsg <- "Did you correctly specify the object `m_MLE`?"
ex() %>% check_object("m_MLE", undefined_msg = "Make sure to not remove `m_MLE`!") %>%check_equal(incorrect_msg=m_MLEmsg)
q_MLEmsg <- "Did you correctly specify the object `q_MLE`?"
ex() %>% check_object("q_MLE", undefined_msg = "Make sure to not remove `q_MLE`!") %>%check_equal(incorrect_msg=q_MLEmsg)
success_msg("Excellent job! When comparing the mean to the variance for the basic count distributions, the variance is smaller for the binomial, equal for the Poisson, and larger for the negative binomial. So comparing the mean to the variance is a quick signal that helps to specify a count distribution.")
```



### Exercise. Fitting a Negative Binomial Distribution

**Assignment Text**

In this assignment, you are asked to fit the negative binomial model to a small set of count data, $\{1,  1,  2,  6,  1,  1,  2,  5, 11\}$.  Because $r$, a parameter of the negative binomial distribution, is a positive real number, it is convenient to use the `optimize` function for maximizing the *reduced* likelihood in order to find its *mle*. It is always advisable to visually confirm the solution by plotting the reduced log-likelihood. 

Note that it is important to check that the sample mean is lower than sample variance for the *mle* of $r$ to be finite - lest the Poisson is the better model. The data are available in an array $\bf x$ and the array $\bf m.vec$ of Exercise \@ref(Ex:CountDataCompression) is also made available.

:::: {.blackbox }
**Instructions**

- Develop the reduced log-likelihood function based on the compressed data in the array $\bf m.vec$.
- The optimization routine will need a range of potential values of $r$. The illustrative code uses a moment estimator (based on techniques that we will cover formally later in [Section 4.1.1 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#S:MS:NonParEst)).
- Determine the value of $r$ that minimizes the reduced log-likelihood using the function [optimize()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/optimize) (a one dimensional optimization function). 
- To check your results, plot the reduced log-likelihood function over a range of $r$. Superimpose a vertical line at the *mle*.
- Check to see whether the sample mean is smaller than the sample variance. Is this consistent with the negative binomial distribution?
::::

<br>

```{r ex="LDA1.2.3.5", type="hint", tut=TRUE}
For the negative binomial, at the *mle*, we have of *r beta* is simply the average. So, we only need to do one dimensional optimization even though there are two parameters. Review [Section 2.4.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:frequency-distributions-mle) for a reminder.
```


```{r ex="LDA1.2.3.5", type="pre-exercise-code", tut=TRUE}
set.seed("1234")
x <- rnbinom(9,2.3,mu=4)
#x <- c(1,  1,  2,  6,  1,  1,  2,  5, 11)
m.vec <- rep(0,max(x)+1)
m.vec[as.integer(names(table(x)))+1] = as.vector(table(x))
```


```{r ex="LDA1.2.3.5", type="sample-code", tut=TRUE}
reduced_loglike<-function(r){
    sum(??*log(dnbinom(0:max(x),??,mu=mean(x))))
}

# Estimator for r
r_moment <- mean(x)^2/(var(x)-mean(x))
r_MLE <- optimize(??,lower=0,upper=3*r_moment,maximum=TRUE)$maximum
beta_MLE <- mean(x)/??
c(r_MLE,beta_MLE)

  # Important to visually check the log-likelihood
plot(r<-(1:(300*r_moment))/100,sapply(??,??),type="l",xlab="m",ylab=expression(l(m,bar(x)/m)))
abline(v=??,col="green")

c(mean(??), var(c(x,??)))
```

```{r ex="LDA1.2.3.5", type="solution", tut=TRUE}
reduced_loglike<-function(r){
    sum(m.vec*log(dnbinom(0:max(x),r,mu=mean(x))))
}

# Estimator for r
r_moment <- mean(x)^2/(var(x)-mean(x))
r_MLE <- optimize(reduced_loglike,lower=0,upper=3*r_moment,maximum=TRUE)$maximum
beta_MLE <- mean(x)/r_MLE
c(r_MLE,beta_MLE)

# Important to visually check the log-likelihood
plot(r<-(1:(300*r_moment))/100,sapply(r,reduced_loglike),type="l",xlab="m",ylab=expression(l(m,bar(x)/m)))
abline(v=r_MLE,col="green")

c(mean(x), var(c(x,mean(x))))
```

```{r ex="LDA1.2.3.5", type="sct", tut=TRUE}
r_MLEmsg <- "Did you correctly specify the object `r_MLE`?"
ex() %>% check_object("r_MLE", undefined_msg = "Make sure to not remove `r_MLE`!") %>%check_equal(incorrect_msg=r_MLEmsg)
beta_MLEmsg <- "Did you correctly specify the object `beta_MLE`?"
ex() %>% check_object("beta_MLE", undefined_msg = "Make sure to not remove `beta_MLEj`!") %>%check_equal(incorrect_msg=beta_MLEmsg)
success_msg("Superb! The importance of maximum likelihood estimation is valuable in actuarial applications. Having familiarity with fundamental concepts such as working with reduced likelihoods and plotting likelihood functions helps to develop a deep appreciation of this estimation approach.")
```



## Other Frequency Distributions

***

In this section, you learn how to:

*    Define the $(a,b,1)$ class of frequency distributions and discuss the importance of the recursive relationship underpinning this class of distributions.
*    Interpret zero truncated and modified versions of the binomial, Poisson, and negative binomial distributions.
*    Compute probabilities using the recursive relationship.

***

#### Video: Other Frequency Distributions {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_pvbam51r&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_7m9q3vs0" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe> 

</center>

#### Overheads: Other Frequency Distributions (Click Tab to View) {-}

<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect24A')">A. Why do we need more frequency distributions?</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24B')">B. Heterogeneous Population</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24C')">C. Visualizing Mixtures</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24D')">D. Mixtures in General</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24E')">E. Zero - Modification</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24F')">F. Zero - Modification of (a,b,0) Class</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24G')">G. The (a,b,1) Class</button>
  <button class="tablinks" onclick="openTab(event, 'Sect24H')">H. Review</button>
  </div>

<div id="Sect24A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=35" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=36" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=37" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=39" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=40" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=42" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24G" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=43" width="100%" height="400"> </iframe>
  </div>
<div id="Sect24H" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=44" width="100%" height="400"> </iframe>
  </div>

### Exercise. The (*a,b*,1) Distribution and its Moments
<img src = "ContributorPics\hurdler.png" width = "65" height = "65" style = "float:right; margin-left: 10px; margin-top: 7px" />
(*As a reminder, when you see this symbol, it means that this exercise is challenging and you may wish to skip it on your first pass through the course.*)


**Assignment Text** 

An earlier exercise used recursions for the $(a,b,0)$ class of distributions. The $(a,b,1)$ features the same recursion but it starts at $k=2$. In this assignment you are given $p_1$, $p_2$ and $p_3$ (in an array $\bf p$) from an $(a,b,1)$ distribution. You are tasked to identify the distribution and compute its mean and variance. 

:::: {.blackbox }
**Instructions**. For this exercise, you may find it useful to review matrix operations in `R`. For one nice resource click [here](https://www.statmethods.net/advstats/matrix.html).


- Setup equation in matrix form and solve for $a$ and $b$. Consider a $2 \times 2$ coefficient matrix $\bf C$ such that
${\bf C} \left(\begin{array}{c}a \\ b \end{array}\right) = \left(\begin{array}{c}p_3/p_2 \\ p_2/p_1 \end{array}\right)$. Write an expression for $\bf C$.
- Invert the matrix $\bf C$ and solve for the vector of coefficients $\left(\begin{array}{c}a \\ b \end{array}\right)$.
- Use the sign of the coefficients to identify the $(a,b,1)$ distribution.
- Use the functional form of this distribution to compute *pmf* up to $p_{100}$ directly.
- From these generated probabilities, determine the mean and the variance.
- From the functional form, use the closed form expressions for this distribution to check your mean and the variance calculations in the prior step. See in particular [Section 18.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-SummaryDistributions.html#the-ab1-class).
::::

<br>


```{r ex="LDA1.2.4.1", type="hint", tut=TRUE}
With small matrices and vectors, try various operations. You will find summaries of (*a,b*,1) distributions in [Section 18.1.s of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-SummaryDistributions.html#the-ab1-classs)
```


```{r ex="LDA1.2.4.1", type="pre-exercise-code", tut=TRUE}
dum <- 3/4
p <- c(dum,dum^2/2,dum^3/3)/log(1-dum)
```


```{r ex="LDA1.2.4.1", type="sample-code", tut=TRUE}
C <- matrix(c(1,??,??,1/2),nrow=2,byrow=TRUE) 

ab <- solve(C)%*%c(??,p[2]/p[1])
# Using the sign of the coefficients identify the (a,b,1) distribution
ab[2]/ab[1]
# Use the functional form of this distribution to compute pmf up to p_100 directly 
p_comp <- ??
  # Check for zero-modification
p[??]-p_comp[??]
# Mean and Variance
c(mu<-sum(p_comp*(??:100)),sum(p_comp*(??:100)^2)-mu^2)
# Check using closed form formulae for the first two moments
c(??,??)
```


```{r ex="LDA1.2.4.1", type="solution", tut=TRUE}
C <- matrix(c(1,1/3,1,1/2),nrow=2,byrow=TRUE) 

ab <- solve(C)%*%c(p[3]/p[2],p[2]/p[1])
# Using the sign of the coefficients identify the (a,b,1) distribution
# Since a>0 and b<0 it is ETNB or Logarithmic; hence compute b/a
ab[2]/ab[1]
# Since above equals -1, r=0 and it is logarithmic
# p_k = -1/log(1-a) a^k/k
p_comp <- -1/log(1-ab[1]) *ab[1]^(k<-(1:100))/(k)
# Check for zero-modification
p[1]-p_comp[1]
# Mean and Variance
c(mu <- sum(p_comp*k),sum(p_comp*k^2)-mu^2)
# Check using formulae of logarithmic moments
c(-1/log(1-ab[1]) *ab[1]/(1-ab[1]),-(ab[1]^2+ab[1]*log(1-ab[1]))/((1-ab[1])^2*(log(1-ab[1]))^2))
```

```{r ex="LDA1.2.4.1", type="sct", tut=TRUE}
Cmsg <- "Did you correctly specify the object `C`?"
ex() %>% check_object("C", undefined_msg = "Make sure to not remove `C`!") %>%check_equal(incorrect_msg=Cmsg)
abmsg <- "Did you correctly specify the object `ab`?"
ex() %>% check_object("ab", undefined_msg = "Make sure to not remove `ab`!") %>%check_equal(incorrect_msg=abmsg)
p_compmsg <- "Did you correctly specify the object `p_comp`?"
ex() %>% check_object("p_comp", undefined_msg = "Make sure to not remove `p_comp`!") %>%check_equal(incorrect_msg=p_compmsg)
success_msg("Terrific! Working with matrices can save considerable time in high-dimensional analytics problems.")
```


## Mixture Distributions

***

In this section, you learn how to:

*    Define a mixture distribution when the mixing component is based on a finite number of sub-groups.
*    Compute mixture distribution probabilities from mixing proportions and knowledge of the distribution of each subgroup.
*    Define a mixture distribution when the mixing component is continuous.

***

####  Video: Mixture Distributions {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_8a24c5ar&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_a5yef0t6" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Mixture Distributions (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect25A')">A. Discrete/Finite Mixtures</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25B')">B. Discrete Mixture Probability Mass Function</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25C')">C. Discrete Mixture Example</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25D')">D. Mixture Moments</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25E')">E. Continuous Mixtures</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25F')">F. Continuous Mixtures II</button>
  <button class="tablinks" onclick="openTab(event, 'Sect25G')">G. Review</button>
  </div>

<div id="Sect25A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=46" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=47" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=48" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=49" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=50" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=51" width="100%" height="400"> </iframe>
  </div>
<div id="Sect25G" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=52" width="100%" height="400"> </iframe>
  </div>



### Exercise. Mixtures of Workers' Compensation Claims

**Assignment Text**

You are analyzing a set of workers' compensation claims (claims that pay in the event of injury at a work-place) and focus on the frequency portion. Suppose that it is known that if claims arise from a low-risk class, such as accountants and actuaries working within "four walls," that the number of claims follows a Poisson distribution with parameter $\lambda=4$. However, if claims arise from a high-risk class, such as roofers and lumberjacks, then the number follows a negative binomial distribution with parameters $r=4$ and $\beta=3$. For a particular firm, you do not know whether it is low or high risk but you do know that probability of being low-risk is $\alpha=0.6$.

In this exercise, we will compare the shape of the mixture distribution to the low and high risk distributions.

:::: {.blackbox }
**Instructions** 

-  Determine the probability mass functions for the low and high risk populations for $k=0, \ldots, 20$ possible claim outcomes.
-  Compute the corresponding probability mass function for the mixture distribution.
-  Plot the mixture distribution with superimposed lines for the low and high risk populations. Use different colors and plotting symbols for the three distributions to help viewers distinguish among them.
-  Determine distribution functions for the low, high, and mixture distributions.
-  Plot the mixture distribution function with superimposed lines for the low and high risk populations.
:::: 

<br>

```{r ex="LDA1.2.5.1", type="hint", tut=TRUE}
See the [appendix of the Loss Data Analytics] (https://openacttexts.github.io/Loss-Data-Analytics/C-SummaryDistributions.html#discrete-distributions) 
   for code on using parameters in R.

```


```{r ex="LDA1.2.5.1", type="sample-code", tut=TRUE}
alpha <- 0.6; lambda <- ??  
r <- 4;       beta <- ?? 
kvec <- 0:20

lowrisk <- dpois(kvec, lambda=lambda)
highrisk <- dnbinom(kvec, prob=?? , size = ?? )
poprisk <- ?? 

plot(kvec, poprisk, ylim = c(0, .2), xlab = "Number of Claims", ylab = "Probability", type = "b", pch = 19)
lines(kvec, lowrisk, col = "blue", type = "b")
lines(kvec, highrisk, col = "red", type = "b", pch = 23)

lowrisk.p <- ppois(kvec, lambda=lambda)
highrisk.p <- pnbinom(kvec, prob=?? , size = ?? )
poprisk.p <- ?? 

plot(kvec, poprisk.p, ylim = c(0, 1), xlab = "Number of Claims", ylab = "Distribution Function", type = "b", pch = 19)
lines(kvec, lowrisk.p, col = "blue", type = "b")
lines(kvec, highrisk.p, col = "red", type = "b", pch = 23)

```



```{r ex="LDA1.2.5.1", type="solution", tut=TRUE}
alpha <- 0.6; lambda <- 4; 
r <- 4;       beta <- 3
kvec <- 0:20

lowrisk <- dpois(kvec, lambda=lambda)
highrisk <- dnbinom(kvec, prob=1/(1+beta), size = r)
poprisk <- alpha*lowrisk + (1-alpha)*highrisk

plot(kvec, poprisk, ylim = c(0, .2), xlab = "Number of Claims", ylab = "Probability", type = "b", pch = 19)
lines(kvec, lowrisk, col = "blue", type = "b")
lines(kvec, highrisk, col = "red", type = "b", pch = 23)

lowrisk.p <- ppois(kvec, lambda=lambda)
highrisk.p <- pnbinom(kvec, prob=1/(1+beta), size = r)
poprisk.p <- alpha*lowrisk.p + (1-alpha)*highrisk.p

plot(kvec, poprisk.p, ylim = c(0, 1), xlab = "Number of Claims", ylab = "Distribution Function", type = "b", pch = 19)
lines(kvec, lowrisk.p, col = "blue", type = "b")
lines(kvec, highrisk.p, col = "red", type = "b", pch = 23)

```



```{r ex="LDA1.2.5.1", type="sct", tut=TRUE}
lambdamsg <- "Did you correctly specify the object `lambda`?"
ex() %>% check_object("lambda", undefined_msg = "Make sure to not remove `lambda`!") %>%check_equal(incorrect_msg=lambdamsg)
betamsg <- "Did you correctly specify the object `beta`?"
ex() %>% check_object("beta", undefined_msg = "Make sure to not remove `beta`!") %>%check_equal(incorrect_msg=betamsg)
highriskmsg <- "Did you correctly specify the object `highrisk`?"
ex() %>% check_object("highrisk", undefined_msg = "Make sure to not remove `highrisk`!") %>%check_equal(incorrect_msg=highriskmsg)
popriskmsg <- "Did you correctly specify the object `poprisk`?"
ex() %>% check_object("poprisk", undefined_msg = "Make sure to not remove `poprisk`!") %>%check_equal(incorrect_msg=popriskmsg)
highrisk.pmsg <- "Did you correctly specify the object `highrisk.p`?"
ex() %>% check_object("highrisk.p", undefined_msg = "Make sure to not remove `highrisk.p`!") %>%check_equal(incorrect_msg=highrisk.pmsg)
poprisk.pmsg <- "Did you correctly specify the object `poprisk.p`?"
ex() %>% check_object("poprisk.p", undefined_msg = "Make sure to not remove `poprisk.p`!") %>%check_equal(incorrect_msg=poprisk.pmsg)

success_msg("Superb! Determining mixture distributions are usually difficult to do by hand but are straightforward with computational tools such as 'R'. Insurance analysts continually fret about unobserved characteristics (such as low versus high risk) and mixture distributions is a tool often used to help quantify these unobserved pieces of information. ")
```




### Exercise. Finite Number of Mixture Distributions

**Assignment Text**

The following describes a "classic" actuarial exam problem. We use this problem to motivate an introduction of more complex techniques for calculating mixture distributions. Unlike classic exam problems designed for hand calculations, these techniques can readily be extended to a large number of unobserved sub-populations.

In a certain town the number of common colds an individual will get in a year follows a Poisson distribution that depends on the individual's age and smoking status:

$$
{\small
\begin{array}{l|cc} \hline
& \text{Proportion of population} & \text{Mean number of colds} \\ \hline
  \text{Children} &        0.3 &          3 \\
\text{Adult Non-Smokers} &        0.6 &          1 \\
\text{Adult Smokers} &        0.1 &          4 \\\hline
\end{array}
}
$$

In this exercise, we will use `R` to calculate the probabilities that a randomly drawn person has a cold in a year.

:::: {.blackbox }
**Instructions** 

-  Create a vector of proportions $\alpha$ and a vector of Poisson parameters $\lambda$.
-  Use the function [dpois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson) to obtain a vector of Poisson probability mass function (pmf) with different means for $k=3$ colds. Then, use the matrix operation [%*%](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/matmult) to obtain the mixture *pmf* as the inner product of the two vectors containing the Poisson pmfs and population percentages.
-  In the same way, use the [ppois()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Poisson) function to compute the probability of at most 3 colds within a year.
-  Now, consider $k=0, \ldots, 8$ colds during a year. For each value of $k$, determine the probability of $k$ colds within a year.
-  Provide a [barplot()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/barplot) of the distribution of number of colds during a year over the range $k=0, \ldots, 8$.
:::: 

<br>

```{r ex="LDA1.2.5.2", type="hint", tut=TRUE}
Take some time to explore the online `R` documentation.

```


```{r ex="LDA1.2.5.2", type="sample-code", tut=TRUE}
alpha <- c(0.3, 0.6, 0.1)
lambda.vec<- ??

# Probability of having 3 common colds in a year
byrisk <- dpois(??, lambda=lambda.vec)
byrisk  %*% alpha

# Probability of at most 3 common colds in a year
ppois(q=3, lambda=lambda.vec) %*% ??

kvec = 0:8
probs = rep(0,length(kvec))
for (index in kvec) {probs[index+1] = ??(index, lambda=lambda.vec) %*% alpha}

barplot(probs, xlab = "Number of Claims", names.arg = kvec)

```


```{r ex="LDA1.2.5.2", type="solution", tut=TRUE}
alpha <- c(0.3, 0.6, 0.1)
lambda.vec<- c(3, 1, 4)
byrisk <- dpois(3, lambda=lambda.vec)
# Probability of having 3 common colds in a year
byrisk  %*% alpha

# Probability of at most 3 common colds in a year
ppois(q=3, lambda=lambda.vec) %*% alpha

kvec = 0:8
probs = rep(0,length(kvec))
for (index in kvec) {probs[index+1] =  dpois(index, lambda=lambda.vec) %*% alpha}

barplot(probs, xlab = "Number of Claims", names.arg = kvec)

```

```{r ex="LDA1.2.5.2", type="sct", tut=TRUE}
lambda.vecmsg <- "Did you correctly specify the object `lambda.vec`?"
ex() %>% check_object("lambda.vec", undefined_msg = "Make sure to not remove `lambda.vec`!") %>%check_equal(incorrect_msg=lambda.vecmsg)
byriskmsg <- "Did you correctly specify the object `byrisk`?"
ex() %>% check_object("byrisk", undefined_msg = "Make sure to not remove `byrisk`!") %>%check_equal(incorrect_msg=byriskmsg)

success_msg("Superb! This exercise explicitly includes only three sub-populations but hopefully it is apparent how it could be extended to a large number of sub-populations. In the next exercise, we consider an infinite number!")
```



### Exercise. Gamma Mixture of Poissons
<img src = "ContributorPics\hurdler.png" width = "65" height = "65" style = "float:right; margin-left: 10px; margin-top: 7px" />

**Assignment Text**

For a population, suppose that each risk has a Poisson number of claims with a parameter $\lambda$ that is specific to that risk (an infinite number of risk classes). We can think of the risk parameter as following a distribution and so is itself random, denoted as a capital $\Lambda$. A mathematically convenient assumption is to assume that the risk parameter follows a gamma distribution. That is, as we have learned from the text, a gamma mixture of Poissons turns out to have a negative binomial distribution. More precisely, if $N|\Lambda \sim$ Poisson$(\Lambda)$ and $\Lambda \sim \text{gamma}(\alpha, \theta)$, then $N \sim \text{Negative Binomial}$ $(r = \alpha, \beta = \theta)$. For example, one can determine the probability mass function of $N$ as

$$
\Pr(N=k) = \int^{\infty}_0 e^{-\lambda} \frac{\lambda^k}{k!} ~ g(\lambda;\alpha, \beta = \theta )~ d \lambda, 
$$

where $g(\cdot;\alpha, \beta = \theta )$ is a gamma density. The proof of this result is in the text; here, we check it using `R`, in the special case of $k=3$, $\alpha =3$, and $\theta = 4$. 

:::: {.blackbox }
**Instructions** 

-  Establish the parameter values for $\alpha =3$ and $\theta = 4$.
-  Express the conditional Poisson mass function as a function of the parameter $\lambda$ (not the number of outcomes $k$) (called "lambda.arg" for the lambda argument in the following sample code).
-  Express the product of the conditional Poisson mass function and the gamma density as a function of $\lambda$.
-  [integrate()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/integrate) this product over values of $\lambda$. Check the result by using the negative binomial probability mass function.
-  Repeat this process using distribution functions in lieu of probability mass functions. Specifically, express the product of the conditional Poisson **distribution** function and the gamma **density** as a function of $\lambda$. Integrate this and check the result using the negative binomial **distribution** function.
::::

<br>

```{r ex="LDA1.2.5.3", type="hint", tut=TRUE}
For the last part of this problem, it is easy to get confused about which is a df and which is a *pmf* (or *pdf*). Remember, we are integrating over different values of lambda and this distribution is determined by the gamma process. So, the gamma stays as a *pdf*.

```


```{r ex="LDA1.2.5.3", type="sample-code", tut=TRUE}
alpha = ??;     theta = ??

pdfPoisson <- function(lambda.arg){dpois(3, lambda=lambda.arg)}
pdfgamPoi <- function(lambda.arg){dgamma(lambda.arg, shape = alpha, scale = theta)*pdfPoisson(lambda.arg)}
integrate(pdfgamPoi, lower = 0, upper = Inf)$value 

dnbinom(3, prob=1/(1+theta), size = alpha)

pdfgamPoi.p <- function(lambda.arg){dgamma(??, shape = alpha, scale = theta)*ppois(3, lambda=??)}
integrate(pdfgamPoi.p, lower = 0, upper = ??)$value 

pnbinom(??, prob=??, size = ??)

```


```{r ex="LDA1.2.5.3", type="solution", tut=TRUE}
alpha = 3
theta = 4

pdfPoisson <- function(lambda.arg){dpois(3, lambda=lambda.arg)}
pdfgamPoi <- function(lambda.arg){dgamma(lambda.arg, shape = alpha, scale = theta)*pdfPoisson(lambda.arg)}
integrate(pdfgamPoi, lower = 0, upper = Inf)$value 

dnbinom(3, prob=1/(1+theta), size = alpha)

pdfgamPoi.p <- function(lambda.arg){dgamma(lambda.arg, shape = alpha, scale = theta)*ppois(3, lambda=lambda.arg)}
integrate(pdfgamPoi.p, lower = 0, upper = Inf)$value 

pnbinom(3, prob=1/(1+theta), size = alpha)

```

```{r ex="LDA1.2.5.3", type="sct", tut=TRUE}
alphamsg <- "Did you correctly specify the object `alpha`?"
ex() %>% check_object("alpha", undefined_msg = "Make sure to not remove `alpha`!") %>%check_equal(incorrect_msg=alphamsg)
thetamsg <- "Did you correctly specify the object `theta`?"
ex() %>% check_object("theta", undefined_msg = "Make sure to not remove `theta`!") %>%check_equal(incorrect_msg=thetamsg)
pdfgamPoi.pmsg <- "Did you correctly specify the object `pdfgamPoi.p`?"
ex() %>% check_object("pdfgamPoi.p", undefined_msg = "Make sure to not remove `pdfgamPoi.p`!") %>%check_equal(incorrect_msg=pdfgamPoi.pmsg)
pnbinommsg <- "Check the parameters of the negative binomial distribution."
ex() %>% check_function("pnbinom") %>% check_result() %>% check_equal(incorrect_msg=pnbinommsg)

success_msg("Excellent! An important strength of this computational approach (as we will see more in the Bayesian section) is that we no longer are limited to simply gamma mixing distributions. Gammas are terrific for getting closed form negative binomial distributions but, if we only want numerical results, then we have many more choices.")
```

## Goodness of Fit

***

In this section, you learn how to:

*    Calculate a goodness of fit statistic to compare a hypothesized discrete distribution to a sample of discrete observations.
*    Compare the statistic to a reference distribution to assess the adequacy of the fit.

***

####  Video: Goodness of Fit {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/25717641/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_svaw2bgx&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_1bvm4vxr" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Goodness of Fit (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect26A')">A. The Goodness of Fit Problem</button>
  <button class="tablinks" onclick="openTab(event, 'Sect26B')">B. Example: Singapore Automobile Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect26C')">C. Fitting a Poisson</button>
  <button class="tablinks" onclick="openTab(event, 'Sect26D')">D. Adequacy of the Poisson Model</button>
  </div>

<div id="Sect26A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=54" width="100%" height="400"> </iframe>
  </div>
<div id="Sect26B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=55" width="100%" height="400"> </iframe>
  </div>
<div id="Sect26C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=56" width="100%" height="400"> </iframe>
  </div>
<div id="Sect26D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap2.pdf#page=57" width="100%" height="400"> </iframe>
  </div>


### Exercise. Goodness of Fit: Zero-Modified Poisson
<img src = "ContributorPics\hurdler.png" width = "65" height = "65" style = "float:right; margin-left: 10px; margin-top: 7px" />

**Assignment Text**

A dataset pertaining to a 1993 portfolio of 7,483 automobile insurance policies from a major Singaporean insurance company that contains several characteristics to explain automobile claim frequency is provided by the General Insurance Association of Singapore. The claims  frequency is contained in the field `Clm_Count`, and the data set has already been read into a data frame called `Insample` and made available to you. You can learn more about the data set at [Singapore Auto Claims](https://instruction.bus.wisc.edu/jfrees/jfreesbooks/Regression%20Modeling/BookWebDec2010/DataDescriptions.pdf) (see description of Table 19: Singapore Auto Claims on page 21).

In this assignment, you are asked to fit a zero-modified Poisson using the *mle* method and test the goodness of fit. In a sense, it is a continuation of the example discussed in [Section 2.7 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:goodness-of-fitt)  where the inadequacy of the Poisson model was observed.

The *mle* of $p_0$ is simply the number of zeros divided by the sample size. The *mle* of $\lambda$ turns out to be the solution of the equation

$$
\frac{{\lambda}}{1-\exp({-\lambda})}=\frac{\sum_{k\geq0} k \cdot m_k}{n-m_0}.
$$

You can learn more about the development of the *mle* below.

`r HideProofTheory('MLECalc.1',"Verify Development of the MLE derivation for zero-modified Poisson")`

**MLE derivation for zero-modified Poisson: ** The zero-modified Poisson is a type of $(a,b,1)$ distribution that has two parameters. This is seen by the form of its *pmf*,
$$
 p_k:=\frac{(1-p_0)}{1-\exp(-\lambda)} \exp(-\lambda) \frac{\lambda^k}{k!}, k=1,2,\ldots,
$$
with $p_0\in[0,1]$ and $\lambda>0$. The *nice* part of this two-parameter family is that the likelihood factorizes into two parts which are  solely a function of exactly a single parameter - one corresponding to $p_0$ and the other to $\lambda$. This is shown below:

$$
L(p_0,\lambda)= \underbrace{\left(p_0^{m_0} (1-p_0)^{n-m_0}\right)}_{\hbox{$p_0$-part}}\underbrace{\left( (1-\exp(-\lambda))^{-(n-m_0)} \lambda^{\sum_{k\geq1} km_k}\right)}_{\hbox{$\lambda$-part}} \underbrace{\left(\prod_{k\geq 1} (k!)^{m_k}\right)^{-1}}_\hbox{Constant} .
$$

The above observation implies maximizing the likelihood with respect to both the parameters can be reduced to two one-variable maximization problems. The first part is precisely a Bernoulli likelihood, the maximization results in the *mle* of $p_0$ given by $\hat{p_0}=m_0/n$. Maximizing the second part implies that the *mle* $\hat{\lambda}$ satisfies,

$$
\frac{\hat{\lambda}}{1-\exp(\hat{-\lambda})}=\frac{\sum_{k\geq0} k \cdot m_k}{n-m_0}.
$$
Observe that the right hand side equals $\bar{x}/(1-\hat{p_0})$. As an aside, the latter being the empirical mean of positive counts and  $\frac{{\lambda}}{1-\exp({-\lambda})}$ its population analog lend a method-of-moments interpretation for the *mle*.  Computing $\hat{\lambda}$ then reduces to solving the equation 
$$
\frac{{\lambda}}{1-\exp({-\lambda})}=\frac{\sum_{k\geq0} k \cdot m_k}{n-m_0}.
$$
Because the left-hand side is a strictly increasing function (show this using calculus and the fact that $e^x\geq 1+x$; otherwise, see the plot of the function below) on $[0,\infty)$ assuming a value of $1$ at $0$ and increasing to infinity, and the right-hand side is at least $1$ (Why?), there is a unique solution to the above equation. In other words, $\hat{\lambda}$ is well-defined. 

```{r fig.height=3,fig.width=3,fig.align="center", echo = FALSE}
plot(lam<-(0:100)/10,lam/(1-exp(-lam)),type="l",xlab=expression(lambda),
     ylab=expression(lambda/(1-exp(-lambda))),ylim=c(0,10))
```

***

</div>

:::: {.blackbox }
**Instructions** 

- To get a feel for the data, start by generating the frequency table.
- Store distinct claim counts observed in array named values.
- Store the frequency of claim counts in $\bf m.vec$.
- Calculate sample mean of claim counts and the *mle* for $p_0$.
- Code the function that provides the framework for determining the *mle* of $\lambda$
- Find the *mle* of  $\lambda$ by solving for the root of the function using  [uniroot()](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/uniroot). (This function searches the interval from lower to upper for a root (i.e., zero) of the function f with respect to its first argument.)
-  Construct a 2 by 1 vector containing the observed and estimated probabilities. The five bins are $\{0\}, \{1\},$ $\{2\}, \{3\}, \{4,5,...\}$.
- Use a barplot to compare observed and estimated probabilities.
- Compute the chi-square statistic and the 95th percentile of the appropriate chi- square distribution.
::::

<br>



```{r ex="LDA1.2.6.1", type="hint", tut=TRUE}
Read through [Section 2.7 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:goodness-of-fitt) and the  instructions carefully. 
```


```{r ex="LDA1.2.6.1", type="pre-exercise-code", tut=TRUE}
InSample <- read.csv("https://raw.githubusercontent.com/OpenActTextDev/LDACourse1/main/Data/SingaporeAuto.csv", header=T,na.strings=c("."),stringsAsFactors=FALSE)
```

```{r ex="LDA1.2.6.1", type="sample-code", tut=TRUE}
table(InSample$??)
# Store distinct claim counts observed in array named values
values <- as.integer(names(table(??)))
# Store the frequency of claim counts in m.vec
m.vec <- as.vector(table(??))
# Calculate sample mean of claim counts
xbar <- sum(??*??)/sum(??)
# MLE for p_0
p_0_MLE <- m.vec[??]/sum(??)
# MLE for lambda
MLE_eqn <- function(lam){
  ifelse(lam==0,??,lam/(1-exp(-lam)))-xbar/(1-p_0_MLE)
}
lambda_MLE <- uniroot(??,c(0,xbar/(1-p_0_MLE)))$root
# Constructing a 2x1 vector containing the observed and estimated probabilities 
# The five bins are {0}, {1}, {2}, {3}, {4,5,...}
data_fit <- cbind(c(m.vec/sum(m.vec),0), c(p_0_MLE,(1-p_0_MLE)/(1-exp(-lambda_MLE))*dpois(values[2:length(values)],lambda_MLE),
(1-p_0_MLE)/(1-exp(-lambda_MLE))*(1-exp(-lambda_MLE)-sum(dpois(values[2:length(values)],lambda_MLE))) ) )
#barplot
barplot(t(data_fit),names.arg=c(values,">=4"),ylab="Frequency",xlab="Count",beside=T,col=c("black","blue"),ylim=c(0,1))
legend(5,0.8, c("Observed", "Fitted"), horiz = T, col=c("black","blue"), fill=c("black","blue"))
# Compute the chi-square statistic
# And 95th-%ile of the appropriate chi- square distribution
c(qchisq(??,??),sum((data_fit%*%c(1,??))^??/data_fit[,??])*sum(??))
```

```{r ex="LDA1.2.6.1", type="solution", tut=TRUE}
table(InSample$Clm_Count)
# Store distinct claim counts observed in array named values
values <- as.integer(names(table(InSample$Clm_Count)))
# Store the frequency of claim counts in m.vec
m.vec <- as.vector(table(InSample$Clm_Count))
# Calculate sample mean of claim counts
xbar <- sum(values*m.vec)/sum(m.vec)
# MLE for p_0
p_0_MLE <- m.vec[1]/sum(m.vec)
# MLE for lambda
MLE_eqn <- function(lam){
  ifelse(lam==0,1,lam/(1-exp(-lam)))-xbar/(1-p_0_MLE)
}
lambda_MLE <- uniroot(MLE_eqn,c(0,xbar/(1-p_0_MLE)))$root
# Constructing a 2x1 vector containing the observed and estimated probabilities 
# The five bins are {0}, {1}, {2}, {3}, {4,5,...}
data_fit <- cbind(c(m.vec/sum(m.vec),0),  c(p_0_MLE,(1-p_0_MLE)/(1-exp(-lambda_MLE))*dpois(values[2:length(values)],lambda_MLE),
(1-p_0_MLE)/(1-exp(-lambda_MLE))*(1-exp(-lambda_MLE)-sum(dpois(values[2:length(values)],lambda_MLE)))  ) )
#barplot
barplot(t(data_fit),names.arg=c(values,">=4"),ylab="Frequency",xlab="Count",beside=T,col=c("black","blue"),ylim=c(0,1))
legend(5,0.8, c("Observed", "Fitted"), horiz = T, col=c("black","blue"), fill=c("black","blue"))
# Compute the chi-square statistic
# And 95th-%ile of the appropriate chi-square distribution
c(qchisq(0.95,2),sum((data_fit%*%c(1,-1))^2/data_fit[,2])*sum(m.vec))
```

```{r ex="LDA1.2.6.1", type="sct", tut=TRUE}
valuesmsg <- "Did you correctly specify the object `values`?"
ex() %>% check_object("values", undefined_msg = "Make sure to not remove `values`!") %>%check_equal(incorrect_msg=valuesmsg)
m.vecmsg <- "Did you correctly specify the object `m.vec`?"
ex() %>% check_object("m.vec", undefined_msg = "Make sure to not remove `m.vec`!") %>%check_equal(incorrect_msg=m.vecmsg)

p_0_MLEmsg <- "Did you correctly specify the object `p_0_MLE`?"
ex() %>% check_object("p_0_MLE", undefined_msg = "Make sure to not remove `p_0_MLE`!") %>%check_equal(incorrect_msg=p_0_MLEmsg)

lambda_MLEmsg <- "Did you correctly specify the object `lambda_MLE`?"
ex() %>% check_object("lambda_MLE", undefined_msg = "Make sure to not remove `lambda_MLE`!") %>%check_equal(incorrect_msg=lambda_MLEmsg)

data_fitmsg <- "Did you correctly specify the object `data_fit`?"
ex() %>% check_object("data_fit", undefined_msg = "Make sure to not remove `data_fit`!") %>%check_equal(incorrect_msg=data_fitmsg)

success_msg("Success! Now compare the above *mle* for lambda with that for the Poisson model (see Section 2.7)")
```


## Contributors {-}

-  **Authors**. **N.D. Shyamalkumar**, The University of Iowa, **Michelle Xia**, Northern Illinois University, and **Edward (Jed) Frees**, University of Wisconsin-Madison and Australian National University, are the principal authors of the initial version of this chapter.
-  **Chapter Maintainers**. Please contact ??? and/or Jed at <jfrees@bus.wisc.edu> for chapter comments and suggested improvements.





