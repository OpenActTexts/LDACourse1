
# Model Selection and Estimation

**Chapter Description**


Chapters 2 and 3 have described how to fit parametric models to frequency and severity data, respectively. This chapter begins with the selection of models. To compare alternative parametric models, it is helpful to summarize data without reference to a specific parametric distribution. Section 4.1 describes nonparametric estimation, how we can use it for model comparisons and how it can be used to provide starting values for parametric procedures. The process of model selection is then summarized in Sections 4.2 and 4.3. Although our focus is on data from continuous distributions, the same process can be used for discrete versions or data that come from a hybrid combination of discrete and continuous distributions. 

Model selection and estimation are fundamental aspects of statistical modeling. To provide a flavor as to how they can be adapted to alternative sampling schemes, Sections 4.4 and 4.5 describes estimation for grouped, censored and truncated data. To see how they can be adapted to alternative models, the chapter closes with Section 4.6 on Bayesian inference, an alternative procedure where the (typically unknown) parameters are treated as random variables.


:::: {.blackbox }

-  Although not needed to go through the tutorials, some users may wish to download the overheads that the videos are based on. <button download><a href="https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/LDA1.Overheads/LDA1.Chap4.pdf">Download Chapter Four overheads as a .pdf file.</a></button>
-  By watching the videos and working through the tutorial exercises, you will get an appreciation for model selection and estimation. For a deeper dive, see the corresponding chapter in the textbook, [Chapter Four of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html).
:::: 


## Nonparametric Inference {#Sec:NonInf}

***

In this section, you learn how to:

- Estimate moments, quantiles, and distributions without reference to a parametric distribution.

***

####  Video: Nonparametric Estimation Tools  {-}


<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_su91og1z&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_bfq6m2gp" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Nonparametric Estimation Tools (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect41A')">A. Nonparametric Estimation</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41B')">B. Moment Estimators</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41C')">C. Empirical Cumulative Distribution Function</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41D')">D. Empirical Example</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41E')">E. Empirical Cumulative Distribution Function of a Toy Example</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41F')">F. Smoothed Empirical Percentiles I</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41G')">G. Smoothed Empirical Percentiles II</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41H')">H. Density Estimators</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41I')">I. Uniform Kernel Density Estimator</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41J')">J. Kernel Density Estimator</button>
  <button class="tablinks" onclick="openTab(event, 'Sect41K')">K. Kernel Density Estimator of a Distribution Function</button>  
  <button class="tablinks" onclick="openTab(event, 'Sect41L')">L. Grouped Data I</button>  
  <button class="tablinks" onclick="openTab(event, 'Sect41M')">M. Grouped Data II</button>  
  </div>

<div id="Sect41A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=3" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=4" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=5" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=6" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=7" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=8" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41G" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=9" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41H" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=10" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41I" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=11" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41J" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=12" width="100%" height="400"> </iframe>
  </div>  
<div id="Sect41K" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=13" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41L" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=14" width="100%" height="400"> </iframe>
  </div>
<div id="Sect41M" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=15" width="100%" height="400"> </iframe>
  </div>  
  



### Exercise. Empirical Distribution Function and Quartiles, Percentiles and Quantiles


**Assignment Text**

In this tutorial, you will find empirical (cumulative) distribution and quantiles for the loss. The Wisconsin Property Fund data has already been read into a data frame called `Insample`. These data consist of claim experience for fund members over the years 2006-2010, inclusive. It includes the claim year `Year` and claim loss `y`.



:::: {.blackbox }
**Instructions**

-  Use the function [ecdf()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/ecdf) to create the empirical (cumulative) distribution for loss amount `y` and plot it.
-  Use the function [quantile()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) to produce sample quantiles corresponding to the given percentiles for each year. Which year has the largest 90th percentile? 
::::

<br>


```{r ex="LDA1.4.1.1_23", type="hint", tut=TRUE}

group_by() takes an existing data and converts it into a grouped data where operations are performed by group.
summarise() outputs a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.
Which `type` should we use for quantile()?

```


```{r ex="LDA1.4.1.1_23", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)
```


```{r ex="LDA1.4.1.1_23", type="sample-code", tut=TRUE}

# Create empirical distribution
empirical <- ??
# Plot empirical distribution
plot(??)

library(dplyr)
# Group data by year and summarise data using quantile
Insample %>% 
  group_by(??) %>% 
  summarise(??)

```


```{r ex="LDA1.4.1.1_23", type="solution", tut=TRUE}

# Create empirical distribution
empirical <- ecdf(Insample$y)
# Plot empirical distribution
plot(empirical, main="", xlab="loss",
     verticals=TRUE, col.points="blue",
     col.hor="red", col.vert="bisque")

library(dplyr)
# Group data by year and summarise data using quantile
Insample %>% 
  group_by(Year) %>% 
  summarise(quantile_each_year=quantile(y, probs=c(0.9), type=6))

```


```{r ex="LDA1.4.1.1_23", type="sct", tut=TRUE}
empmsg <- "Did you correctly specify the object `empirical`?"
ex() %>% check_object("empirical", undefined_msg = "Make sure to not remove `empirical`!") %>% check_equal(incorrect_msg=empmsg)
plot1msg<-"Use `plot` on the `empirical` object"
ex() %>% check_function("plot", not_called_msg=plot1msg) %>% check_result() %>% check_equal(incorrect_msg=plot1msg)
success_msg("Well done! As your analysis shows, Year 2007 has the largest 90th percentiles.")
```


### Exercise. Density Estimators

**Assignment Text**

Nonparametric density estimators, such as the kernel estimator, are regularly used in practice. In this tutorial, We will use the Wisconsin Property Fund data, which already has been read into a data frame called `Insample`, to plot a histogram and overlay density curves with various density estimators. 

:::: {.blackbox }
**Instructions**

-  Plot a histogram of logarithmic loss amount `y`.
-  Use the function [density()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/density) to compute kernel density estimates.
-  Overlay density curves on top of the histogram: 
    - using a blue thick curve to represent a Gaussian kernel density where the bandwidth was selected automatically using an ad hoc rule based on the sample size and volatility of these data, 
    - using a green thick curve to represent a Gaussian kernel density with a bandwidth equal to 1,
    - using a red thick curve to represent a triangular kernel density with a bandwidth of 0.1.
:::: 

<br>


```{r ex="LDA1.4.1.1_4", type="hint", tut=TRUE}

The syntax `log(data$x)` produces the logarithm of variable `x` in the data frame `data`.

```


```{r ex="LDA1.4.1.1_4", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)
```


```{r ex="LDA1.4.1.1_4", type="sample-code", tut=TRUE}

# Plot histogram
hist(??, freq=FALSE)
# Gaussian kernel density - blue curve
lines(density(??, col="blue", lwd=2)
# Gaussian kernel density with a bandwidth equal to 1 - green curve
lines(density(??, bw=??), col="green", lwd=2)
# Triangular kernel density with a bandwidth equal to 0.1 - red curve
lines(density(??, kernel=??, bw=??), col="red", lwd=2)

```


```{r ex="LDA1.4.1.1_4", type="solution", tut=TRUE}

# Plot histogram
hist(log(Insample$y), freq=FALSE)
# Gaussian kernel density - blue curve
lines(density(log(Insample$y)), col="blue", lwd=2)
# Gaussian kernel density a bandwidth equal to 1 - green curve
lines(density(log(Insample$y), bw=1), col="green", lwd=2)
# Triangular kernel density a bandwidth equal to 0.1 - red curve
lines(density(log(Insample$y), kernel="triangular", bw=0.1), col="red", lwd=2)

```


```{r ex="LDA1.4.1.1_4", type="sct", tut=TRUE}
hist1msg<-"Use the function `hist` on the logarithm of variable `y` in the data frame `Insample`"
ex() %>% check_function("hist", not_called_msg=hist1msg) %>% check_result() %>% check_equal(incorrect_msg=hist1msg)
density1msg<-"Use the function `density` on the logarithm of variable `y` in the data frame `Insample`"
ex() %>% check_function("density", not_called_msg=density1msg) %>% check_result() %>% check_equal(incorrect_msg=density1msg)
success_msg("Excellent! This is a very beautiful graphic with colors! We observed different density curves.")
```


### Exercise. Comparing Poisson and Negative Binomial Distributions

In this tutorial, you will model claim count data and decide whether a Poisson$(\lambda)$ or a negative binomial$(r=3,p)$ better fits the data. Because of the number of steps involved, we split this tutorial into several distinct stages.

**Assignment Text**

The raw data are the number of claims filed by $n=100$ policyholders.  Recall that claim count data are discrete and take values $0, 1, 2, \dots$. So, we consider two discrete probability models. You will apply tools learned in Section \@ref(Sec:NonInf) to make a decision on which model fits these data better.

We start with exploratory data analysis, making graphical and numeric summaries of data before fitting a model.

:::: {.blackbox }
**Instructions** 

- Make a [table](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/table) of the claims counts.
- Make a [barplot](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/barplot) to graphically display the claims counts.
- Compute the [mean](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean) and [variance](http://www.r-tutor.com/elementary-statistics/numerical-measures/variance) of the claims count distribution.  Based on these, do you think a Poisson or a negative binomial model might be better?
:::: 

<br>


```{r ex="LDA4.2.1.1", type="hint", tut=TRUE}
The output from the `table()` function can be used as an argument in the `barplot()` function
```


```{r ex="LDA4.2.1.1", type="pre-exercise-code", tut=TRUE}
set.seed(4)
claims = rnbinom(n = 100, size = 3, prob = 0.5)

```


```{r ex="LDA4.2.1.1", type="sample-code", tut=TRUE}
table(claims)
barplot(??, xlab="# Claims")

mean(claims)
??(claims)

```


```{r ex="LDA4.2.1.1", type="solution", tut=TRUE}
table(claims)
barplot(table(claims), xlab="# Claims")
mean(claims)
var(claims)
```

```{r ex="LDA4.2.1.1", type="sct", tut=TRUE}

barplot1msg<-"Use `barplot` on the `table(claims)` object?"
ex() %>% check_function("barplot", not_called_msg=barplot1msg) %>% check_result() %>% check_equal(incorrect_msg=barplot1msg)
var1msg<-"Use `var` on the `claims` object?"
ex() %>% check_function("var", not_called_msg=var1msg) %>% check_result() %>% check_equal(incorrect_msg=var1msg)
success_msg("And you're off!  By the way - did you label your axes?")
```


Observe that the distribution is heavily right-skewed, and the sample variance is much larger than the sample mean.  You may recall that if $X \sim$ Poisson$(\lambda)$,
$$
\mathrm{E}(X) = \mathrm{Var}(X) = \lambda.
$$
We would expect the sample mean and sample variance to be roughly equal if the claims data came from the Poisson... our first clue this distribution may not be best!

:::: {.blackbox }
**Instructions** 

- Fit the Poisson distribution by estimating $\lambda$ using maximum likelihood estimation.
- Make a $qq$ plot comparing the observed data to the fitted Poisson.  

Fitting the distribution involves estimating the parameter $\lambda$.  Do you recall the maximum likelihood estimator for a Poisson?  If not, this is shown in the companion video for this tutorial.
:::: 

<br>

```{r ex="LDA4.2.1.2", type="pre-exercise-code", tut=TRUE}
set.seed(4)
claims = rnbinom(n = 100, size = 3, prob = 0.5)
#Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T,
#                      na.strings=c("."), stringsAsFactors=FALSE)
```

```{r ex="LDA4.2.1.2", type="hint", tut=TRUE}
Remember the MLE for a Poisson is just the sample mean!
  
Should you brush up on the  [quantile](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/quantile) function?
```

```{r ex="LDA4.2.1.2", type="sample-code", tut=TRUE}
lambda = ??
lambda

## make the quantiles
p=seq(0.01,0.99, by=0.01)
qp = qpois(p, lambda=3)

## compute the quantiles
qclaims = quantile(??, p)

## Make a plot (and label axes!)
plot(??, ??, ylim=c(0,12),xlim=c(0,12), xlab="Model Quantiles", ylab="Empirical Quantiles")
abline(0,1)

```


```{r ex="LDA4.2.1.2", type="solution", tut=TRUE}
lambda = mean(claims)
lambda

## make the quantiles
pr=seq(0.01,0.99, by=0.01)
qp = qpois(pr, lambda=3)

## compute the quantiles
qclaims = quantile(claims, pr)

## Make a plot (and label axes!)
plot(qp, qclaims, ylim=c(0,12),xlim=c(0,12), xlab="Model Quantiles", ylab="Empirical Quantiles")
abline(0,1)
```

```{r ex="LDA4.2.1.2", type="sct", tut=TRUE}
lambdamsg <- "Did you correctly specify the object `lambda`?"
ex() %>% check_object("lambda", undefined_msg = "Make sure to not remove `lambda`!") %>% check_equal(incorrect_msg=lambdamsg)

qclaimsmsg <- "Did you correctly specify the object `qclaims`?"
ex() %>% check_object("qclaims", undefined_msg = "Make sure to not remove `qclaims`!") %>% check_equal(incorrect_msg=qclaimsmsg)

plot2msg<-"Use `plot` on the `qp` object"
ex() %>% check_function("plot", not_called_msg=plot2msg) %>% check_result() %>% check_equal(incorrect_msg=plot2msg)

success_msg("Nice!  But it's not a *great* fit, is it?")
```

Do you notice how the empirical quantiles do not match the model for some of the larger values in the right tail?  This plot says the empirical quantiles are too large, that's why they are above the diagonal line $y=x$.  Our claims data are too right-skewed, and that's why the $qq$ plot seems off for larger quantiles.  Might the negative binomial might be more promising?

:::: {.blackbox }
**Instructions** 

- Fit the negative binomial by using the method of moments estimator to estimate $p$.
- Make a $qq$ plot comparing the observed data to the fitted negative binomial.
::::

To fit the Negative Binomial($r=3, \beta$), we need to estimate the parameter $\beta$ using the method of moments.  Recall that the probability mass function for a negative binomial ([Sec 2.2.3.3 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Frequency-Modeling.html#S:important-frequency-distributions)) is
\[
\Pr(X = k) = \frac{(k+r-1)!}{k!(r-1)!}\left(\frac{1}{1+\beta}\right)^r \left(\frac{\beta}{1+\beta} \right)^k, \hspace{5mm} k=0, 1, ...
\]
with parameters $r$ and $\beta$.  Also recall that in the method of moments we equate the $t^{th}$ sample moment and $t^{th}$ model moment, 
$$ 
\mathrm{E}(X^t) = \frac{1}{n}\sum_{i=1}^{n}x_{i}^t,
$$
where $t=1, 2, ...$.  If we use $t=1$ to equate the first moments, we get the expression
$$  
\mathrm{E}(X) = r\beta = 3\beta = \overline{x} = 3
$$
Solving the above for $\beta$, we see $\hat{\beta} = 1$.  Therefore the Negative Binomial($r=3, \beta=1$) is the fitted model.  

Next, we use `R` to fit this and make a $qq$ plot.  It is important to understand that distributions are not uniquely parameterized, and there can be different conventions across fields, textbooks, and software.  Indeed, in `R` the function \texttt{qnbinom} defines the negative binomial as
\[
\Pr(X = k) = \frac{(k+n-1)!}{(n-1)!k!}p^n(1-p)^k, \hspace{5mm} k=0, 1, ...
\]

This matches the definition in the textbook by equating probability $p = \frac{1}{1+\beta}$, and size $r=n$.  It is very important to carefully consider the parameterization of any distribution you use in `R`.


```{r ex="LDA4.2.1.3", type="pre-exercise-code", tut=TRUE}
set.seed(4)
claims = rnbinom(n = 100, size = 3, prob = 0.5)
#Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T,
#                      na.strings=c("."), stringsAsFactors=FALSE)
```



```{r ex="LDA4.2.1.3", type="hint", tut=TRUE}
Should you brush up on the  [qnbinom](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/qnbinom) function?  It can be easy to enter the wrong parameters for a distribution, especially if the names or definitions of the parameters are not universally agreed upon.  Always check!
```

```{r ex="LDA4.2.1.3", type="sample-code", tut=TRUE}
p = ??
pr=seq(0.01,0.99, by=0.01)
qb = qnbinom(pr, size=??, prob = p)

## compute the quantiles
qclaims = quantile(claims, pr)

plot(??, ??, ylim=c(0,12), xlim=c(0,12), ylab="Empirical Quantiles", xlab="Model Quantiles")
abline(0,1)

```


```{r ex="LDA4.2.1.3", type="solution", tut=TRUE}
p = 0.5
pr=seq(0.01,0.99, by=0.01)
qb = qnbinom(pr, size=3, prob = p)

## compute the quantiles
qclaims = quantile(claims, pr)

plot(qb, qclaims, ylim=c(0,12), xlim=c(0,12), ylab="Empirical Quantiles", xlab="Model Quantiles")
abline(0,1)
```

```{r ex="LDA4.2.1.3", type="sct", tut=TRUE}
pmsg <- "Did you correctly specify the object `p`?"
ex() %>% check_object("p", undefined_msg = "Make sure to not remove `p`!") %>% check_equal(incorrect_msg=pmsg)

qbmsg <- "Did you correctly specify the object `qb`?"
ex() %>% check_object("qb", undefined_msg = "Make sure to not remove `qb`!") %>% check_equal(incorrect_msg=qbmsg)

plot3msg<-"Use `plot` on the `qb` and `qclaims` objects"
ex() %>% check_function("plot", not_called_msg=plot3msg) %>% check_result() %>% check_equal(incorrect_msg=plot3msg)

success_msg("This seems to be a better fit than Poisson.")
```





<!-- ```{r child = './Quizzes/Quiz41.html', eval = QUIZ} -->
<!-- ``` -->


## Tools for Model Selection

***

In this section, you learn how to:

-  Summarize the data graphically without reference to a parametric distribution.
-  Determine measures that summarize deviations of a parametric from a nonparametric fit.
-  Use nonparametric estimators to approximate parameters that can be used to start a parametric estimation procedure.

***

####  Video: Tools  for Model Selection {-}


<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_tcqdwc0i&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_jrvuguzf" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Tools  for Model Selection (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect421A')">A. Comparing Distribution and Density Functions</button>
  <button class="tablinks" onclick="openTab(event, 'Sect421B')">B. PP Plot</button>
  <button class="tablinks" onclick="openTab(event, 'Sect421C')">C. QQ Plot</button>
  <button class="tablinks" onclick="openTab(event, 'Sect421D')">D. Goodness-of-Fit Test</button>
  <button class="tablinks" onclick="openTab(event, 'Sect421E')">E. Kolmogorov-Smirnov Test</button>
  <button class="tablinks" onclick="openTab(event, 'Sect421F')">F. Chi-Square (χ2) Test</button>
  </div>

<div id="Sect421A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=18" width="100%" height="400"> </iframe>
  </div>
<div id="Sect421B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=19" width="100%" height="400"> </iframe>
  </div>
<div id="Sect421C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=20" width="100%" height="400"> </iframe>
  </div>
<div id="Sect421D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=21" width="100%" height="400"> </iframe>
  </div>
<div id="Sect421E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=22" width="100%" height="400"> </iframe>
  </div>
<div id="Sect421F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=23" width="100%" height="400"> </iframe>
  </div>
  
### Exercise. Probability-Probability ($pp$) plot {#Ex:PPPlot}
<img src = "ContributorPics\hurdler.png" width = "65" height = "65" style = "float:right; margin-left: 10px; margin-top: 7px" />

**Assignment Text**

Probability-probability ($pp$) plots can be used to corroborate the selection of parametric models. The Wisconsin Property Fund data has already been read into a data frame called `Insample` and we created subset data which contains the positive loss data called `positive_loss`. 

In this tutorial, you will generate $pp$ plots with Gaussian and gamma distribution. Based on the $pp$ plots, which parametric fitted model is closer to the empirical distribution?

:::: {.blackbox }
**Instructions**

-  Use the function [glm(response~1,data=,family=Gamma(link=log))](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) to fit generalized linear model (*glm*) with gamma family.
    -  Find the shape and rate parameters from the fitted gamma *glm*. (No action required on your part.)
-  Use the function [glm(response~1,data=,family=gaussian(link=log))](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) to fit generalized linear model (*glm*) with Gaussian distribution.
    -  Find the mean and standard deviation from the fitted Gaussian *glm*. (No action required on your part.)
-  Use the function [ecdf()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/ecdf) to create the empirical distribution for positive loss.
-  Use the function [pgamma()](https://www.rdocumentation.org/packages/Rlab/versions/2.15.1/topics/Gamma) to create distribution function for the gamma distribution. Generate a $pp$ plot for the positive loss amount `y` using the gamma distribution. 
-  Use the function [pnorm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Normal) to create distribution function for the Gaussian distribution.  Generate a $pp$ plot for the positive loss amount `y` using Gaussian  distribution. 

:::: 

<br>

```{r ex="LDA1.4.1.2_1", type="hint", tut=TRUE}

We show the code that uses *glm* to find parameters.

```


```{r ex="LDA1.4.1.2_1", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)
library(MASS)
positive_loss <- subset(Insample, y>0)
```


```{r ex="LDA1.4.1.2_1", type="sample-code", tut=TRUE}
# Fit gamma
fit_gamma <- glm(formula=??, data=??, family=Gamma(link=log))
theta <- exp(coef(fit_gamma))*gamma.dispersion(fit_gamma) 
alpha <- 1/gamma.dispersion(fit_gamma)
# Fit Gaussian
fit_gaussian <- glm(formula=??, data=??, family=gaussian(link=log))
mu <- exp(coef(fit_gaussian))  
sigma <- sqrt(summary(fit_gaussian)$dispersion) 
# Create empirical distribution
empirical <- ??
# gamma pp plot
gamma <- pgamma(??, shape=alpha, scale=theta)
plot(??, gamma, xlab="Empirical DF", ylab="Gamma DF", cex=1)
abline(0,1)
# gaussian pp plot
gaussian <- pnorm(??, mean=mu, sd=sigma)
plot(??, gaussian, xlab="Empirical DF", ylab="Gaussian DF", cex=1)
abline(0,1)

```


```{r ex="LDA1.4.1.2_1", type="solution", tut=TRUE}
# Fit gamma
fit_gamma <- glm(formula=positive_loss$y~1, 
                 data=positive_loss, 
                 family=Gamma(link=log))
theta <- exp(coef(fit_gamma))*gamma.dispersion(fit_gamma) #mu=theta/alpha
alpha <- 1/gamma.dispersion(fit_gamma)

# Fit gaussian
fit_gaussian <- glm(positive_loss$y~1, 
                 data=positive_loss, 
                 family=gaussian(link=log))
mu <- exp(coef(fit_gaussian)) # or mean(positive_loss$y)
sigma <- sqrt(summary(fit_gaussian)$dispersion) # or sd(positive_loss$y) slightly different

# Creat empirical distribution
empirical <- ecdf(positive_loss$y)

# gamma pp plot
gamma <- pgamma(positive_loss$y, shape=alpha, scale=theta)
plot(empirical(positive_loss$y), gamma,  # empirical() returns the percentiles for positive_loss$y
     xlab="Empirical DF", ylab="Gamma DF", cex=1)
abline(0,1)

# gaussian pp plot
gaussian <- pnorm(positive_loss$y, mean=mu, sd=sigma)
plot(empirical(positive_loss$y), gaussian, 
     xlab="Empirical DF", ylab="Gaussian DF", cex=1)
abline(0,1)


```


```{r ex="LDA1.4.1.2_1", type="sct", tut=TRUE}
fit_gammamsg <- "Did you correctly specify the object `fit_gamma`?"
ex() %>% check_object("fit_gamma", undefined_msg = "Make sure to not remove `fit_gamma`!") %>% check_equal(incorrect_msg=fit_gammamsg)

fit_gaussianmsg <- "Did you correctly specify the object `fit_gaussian`?"
ex() %>% check_object("fit_gaussian", undefined_msg = "Make sure to not remove `fit_gaussian`!") %>% check_equal(incorrect_msg=fit_gaussianmsg)

empiricalmsg <- "Did you correctly specify the object `empirical`?"
ex() %>% check_object("empirical", undefined_msg = "Make sure to not remove `empirical`!") %>% check_equal(incorrect_msg=empiricalmsg)

gammamsg <- "Did you correctly specify the object `gamma`?"
ex() %>% check_object("gamma", undefined_msg = "Make sure to not remove `gamma`!") %>% check_equal(incorrect_msg=gammamsg)

gaussianmsg <- "Did you correctly specify the object `gaussian`?"
ex() %>% check_object("gaussian", undefined_msg = "Make sure to not remove `gaussian`!") %>% check_equal(incorrect_msg=gaussianmsg)

success_msg("Good job! The *pp* plot is able to compare cumulative probabilities under two models. For these data, the gamma distribution is closer to the empirical distribution and so provides a better representation of the data.")
```

**Editorial Note**. This exercise uses the [glm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) function to produce maximum likelihood estimates of certain classes of distributions including the gamma and Gaussian (normal). [Section 3.5 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-Severity.html#S:MaxLikeEstimation) employs a comparable function, [vlgm()](https://www.rdocumentation.org/packages/VGAM/versions/1.1-5/topics/vglm). These functions are even more useful when employed in regression modeling contexts. Although regression modeling is outside the scope of this course, the exercise provides an initial exposure to these functions that are very useful in actuarial practice.


### Exercise. Quantile-Quantile ($qq$) plot
  
**Assignment Text**

The $pp$ plot shows *cumulative* distribution functions, discrepancies can sometimes be difficult to detect where a fitted parametric distribution is deficient. As an alternative, it is common to use a quantile-quantile ($qq$) plot. The Wisconsin Property Fund data has already been read into a data frame called `Insample` and we also created a subset data which includes only positive loss data called `positive_loss`. In this exercise, parameter estimates are determined by the method of moments, in contrast to the maximum likelihood approach (with the `glm` function) taken in Exercise \@ref(Ex:PPPlot). This is approach is simpler and demonstrates an alternative method. Recall that we used the method of moments for the gamma distribution in Exercise \@ref(Ex:VisualGamma). 

In this tutorial, you will generate $qq$ plots with Gaussian and gamma distribution. Based on the $qq$ plot, which parametric fitted model is closer to the empirical distribution?

:::: {.blackbox }
**Instructions**

-  Find sample size, mean, variance, and standard deviation for the positive loss. 
-  Determine the method of moments estimators for the gamma distribution.
-  Calculate `p_value`, a list of probabilities used for plotting quantiles.  
-  Use the function [qnorm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Normal) to create the quantiles for the Gaussian distribution. Generate a $qq$ plot for the positive loss amount `y` using the Gaussian distribution.
-  Use the function [qgamma()](https://www.rdocumentation.org/packages/Rlab/versions/2.15.1/topics/Gamma) to create the quantiles for the gamma distribution. Generate a $qq$ plot for the positive loss amount `y` using the gamma distribution. 

:::: 

<br>

```{r ex="LDA1.4.1.2_2", type="hint", tut=TRUE}
You can directly use the sample mean and standard deviation to find parameters for the Gaussian (normal) distribution.

```


```{r ex="LDA1.4.1.2_2", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)
positive_loss <- subset(Insample, y>0)

```


```{r ex="LDA1.4.1.2_2", type="sample-code", tut=TRUE}
# Positive loss number, mean and standard deviation
n <- length(??)
mean <- ??
?? <- sd(positive_loss$y)
# gamma scale and shape parameter estimate
theta = sd^2/mean
alpha = ??

# Probabilities
p_values <- seq(1/(n+1), n/(n+1), by=1/(n+1))

# Gaussian QQ Plot
gaussian_quantiles <- qnorm(p_values, ??, ??)
plot(sort(gaussian_quantiles), sort(positive_loss$y), 
     xlab = 'Fitted Quantiles from Gaussian Distribution', 
     ylab = 'Empirical Quantiles of Positive Loss', 
     main = 'Gaussian QQ Plot of Positive Loss')
abline(0,1)

# Gamma QQ Plot
gamma_quantiles <- qgamma(??)
plot(sort(gamma_quantiles), sort(positive_loss$y), 
     xlab = 'Fitted Quantiles from Gamma Distribution', 
     ylab = 'Empirical Quantiles of Positive Loss', 
     main = 'Gamma QQ Plot of Positive Loss')
abline(0,1)

```


```{r ex="LDA1.4.1.2_2", type="solution", tut=TRUE}
# Positive loss number, mean and standard deviation
n <- length(positive_loss$y)
mean <- mean(positive_loss$y)
sd <- sd(positive_loss$y)
# gamma scale and shape parameter estimate
theta = sd^2/mean
alpha = mean^2/sd^2 

# Probabilities
p_values <- seq(1/(n+1), n/(n+1), by=1/(n+1))

# Gaussian QQ Plot
gaussian_quantiles <- qnorm(p_values, mean, sd)
plot(sort(gaussian_quantiles), sort(positive_loss$y), 
     xlab = 'Fitted Quantiles from Gaussian Distribution', 
     ylab = 'Empirical Quantiles of Positive Loss', 
     main = 'Gaussian QQ Plot of Positive Loss')
abline(0,1)

# Gamma QQ Plot
gamma_quantiles <- qgamma(p_values, shape = alpha , scale = theta)
plot(sort(gamma_quantiles), sort(positive_loss$y), 
     xlab = 'Fitted Quantiles from Gamma Distribution', 
     ylab = 'Empirical Quantiles of Positive Loss', 
     main = 'Gamma QQ Plot of Positive Loss')
abline(0,1)

```


```{r ex="LDA1.4.1.2_2", type="sct", tut=TRUE}
nmsg <- "Did you correctly specify the object `n`?"
ex() %>% check_object("n", undefined_msg = "Make sure to not remove `n`!") %>% check_equal(incorrect_msg=nmsg)

meanmsg <- "Did you correctly specify the object `mean`?"
ex() %>% check_object("mean", undefined_msg = "Make sure to not remove `mean`!") %>% check_equal(incorrect_msg=meanmsg)

alphamsg <- "Did you correctly specify the object `alpha`?"
ex() %>% check_object("alpha", undefined_msg = "Make sure to not remove `alpha`!") %>% check_equal(incorrect_msg=alphamsg)

gaussian_qmsg <- "Did you correctly specify the object `gaussian_quantiles`?"
ex() %>% check_object("gaussian_quantiles", undefined_msg = "Make sure to not remove `gaussian_quantiles`!") %>% check_equal(incorrect_msg=gaussian_qmsg)

gamma_qmsg <- "Did you correctly specify the object `gamma_quantiles`?"
ex() %>% check_object("gamma_quantiles", undefined_msg = "Make sure to not remove `gamma_quantiles`!") %>% check_equal(incorrect_msg=gamma_qmsg)

success_msg("Good job! The *qq* plot compares two fitted models through their quantiles. Again, the gamma is better fit.")
```

***


### Exercise.  Kolmogorov-Smirnov statistic

**Assignment Text**

For reporting results, it can be effective to supplement the graphical displays with selected statistics that summarize model goodness of fit. For background, you can learn more about the Kolmogorov-Smirnov statistic and other goodness of fit statistics in 
[Section 4.1.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#S:MS:ToolsModelSelection).

The Wisconsin Property Fund data has already been read into a data frame called `Insample`. Previously, we also created a subset, `positive_loss`, which included only positive losses. Perform Kolmogorov-Smirnov tests.

:::: {.blackbox }
**Instructions**

-  Compare the empirical positive loss amount `y` distribution to gamma distribution by performing the Kolmogorov-Smirnov test.
-  Use the function [ks.test](https://www.rdocumentation.org/packages/dgof/versions/1.2/topics/ks.test) to perform Kolmogorov-Smirnov test.
:::: 

<br>

```{r ex="LDA1.4.1.2_3", type="hint", tut=TRUE}

Take some time to explore the online `R` documentation. `theta`, `alpha`, and `gamma` can be found in the previous question. 

```

```{r ex="LDA1.4.1.2_3", type="pre-exercise-code", tut=TRUE}
library(MASS)
library(stats)

Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)

# Subset for positive loss
positive_loss <- subset(Insample, y>0)

# Fit gamma
fit_gamma <- glm(positive_loss$y~1, 
                 data=positive_loss, 
                 family=Gamma(link=log))
theta <- exp(coef(fit_gamma))*gamma.dispersion(fit_gamma) #mu=theta/alpha
alpha <- 1/gamma.dispersion(fit_gamma)

gamma <- pgamma(positive_loss$y, shape=alpha, scale=theta)

```

```{r ex="LDA1.4.1.2_3", type="sample-code", tut=TRUE}

# Performs Kolmogorov-Smirnov tests
ks.test(??, "pgamma", ??, ??)

# Performs Kolmogorov-Smirnov tests
ks.test(??, ??)

```


```{r ex="LDA1.4.1.2_3", type="solution", tut=TRUE}

# Performs Kolmogorov-Smirnov tests
ks.test(positive_loss$y, "pgamma", theta, alpha)

# Performs Kolmogorov-Smirnov tests
ks.test(positive_loss$y, gamma)

```


```{r ex="LDA1.4.1.2_3", type="sct", tut=TRUE}
ks.test1msg<-"Use `ks.test` on the `positive_loss$y` object"
ex() %>% check_function("ks.test", not_called_msg=ks.test1msg) %>% check_result() %>% check_equal(incorrect_msg=ks.test1msg)

success_msg("According to the Kolmogorov-Smirnov test, the two distribution functions are not equal. We have seen some deviations from the *qq* graph and *pp* graph. We need to find a better parametric model.")
```


## Model Selection: Likelihood Ratio Tests and Goodness of Fit


####  Video: Likelihood Ratio Tests and Goodness of Fit {-}


<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_sepee6wr&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ud4hj85k" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Likelihood Ratio Tests and Goodness of Fit (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect422A')">A. Likelihood Ratio Test</button>
  <button class="tablinks" onclick="openTab(event, 'Sect422B')">B. Likelihood Ratio Test Process</button>
  <button class="tablinks" onclick="openTab(event, 'Sect422C')">C. Information Criteria: Exam STAM Version</button>
  <button class="tablinks" onclick="openTab(event, 'Sect422D')">D. Information Criteria: Alternative Version</button>
  </div>

<div id="Sect422A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=26" width="100%" height="400"> </iframe>
  </div>
<div id="Sect422B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=27" width="100%" height="400"> </iframe>
  </div>
<div id="Sect422C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=28" width="100%" height="400"> </iframe>
  </div>
<div id="Sect422D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=29" width="100%" height="400"> </iframe>
  </div>
  
  

***


  

<!-- ```{r child = './Quizzes/Quiz42.html', eval = QUIZ} -->
<!-- ``` -->

## Estimation using Modified Data: Nonparametric Approach

***

In this section, you learn how to:

-  Describe grouped and censored truncated data.
-  Estimate distributions nonparametrically based on grouped and censored data.

***

####  Video: Nonparametric Estimation using Modified Data {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_2pposnie&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_156eapyx" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Nonparametric Estimation using Modified Data (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect431A')">A. Grouped Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect431B')">B. Censored Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect431C')">C. Kaplan-Meier Product Limit Estimator</button>
  </div>

<div id="Sect431A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=31" width="100%" height="400"> </iframe>
  </div>
<div id="Sect431B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=32" width="100%" height="400"> </iframe>
  </div>
<div id="Sect431C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=33" width="100%" height="400"> </iframe>
  </div>
  
### Exercise. Kaplan-Meier Estimation based on Censored Data {#Ex:KMEstimation}

**Assignment Text**

This exercise is based on the  Wisconsin Property Fund data. These data consist of claim experience for fund members over the years 2006-2010, inclusive. A subset which includes only positive loss data has been read into a data frame called `positive_loss`. In part because the units of observations are governmental units, there are no upper limits on these policies. So, simply to illustrate coding for censored data, in this exercise we impose an upper limit of 50,000 on the first 100 claims as well as 100,000 on the remainder. (As an analyst, this part of the exercise could be useful to you as you test the sensitivity of policy limits.)

After adding censoring information to the data, we use Kaplan-Meier technique to estimate the claims survival distribution (recall that the survival function is one minus the distribution function).


:::: {.blackbox }
**Instructions**

-  Introduce policy limits as part of the `positive_loss` data.
-  Create a new variable, `AmountPaid`, that is the smaller of the policy limit and the claim amount. In the following sample code, we show how to use the  [dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) data wrangling package. 
-  Also create a binary variable, `UnCensored `, that is one if the claim is less than or equal to the policy limit, and zero otherwise.
-  To determine the Kaplan-Meier product limit estimator, use [survfit()](https://www.rdocumentation.org/packages/survival/versions/2.11-4/topics/survfit) to estimate the survival function for the right-censored empirical distribution for the positive loss amount `y`. Plot the result.
::::

<br>

```{r ex="LDA1.4.3.2_1", type="hint", tut=TRUE}

We will use `dplyr` and traditional methods to perform data wrangling.

```


```{r ex="LDA1.4.3.2_1", type="pre-exercise-code", tut=TRUE}
Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)
# Subset for positive loss
positive_loss <- subset(Insample, y>0)

```


```{r ex="LDA1.4.3.2_1", type="sample-code", tut=TRUE}

# Add policy limit for positive loss data
positive_loss$PolicyLimit <- ??
positive_loss$PolicyLimit[1:100] <- ??

# Add AmountPaid column: using dplyr 
library(dplyr)
positive_loss_limit <- positive_loss %>% rowwise() %>% mutate(AmountPaid=min(y, ??))

# Add UnCensored column: traditional method
positive_loss_limit$UnCensored <- ??

# KM estimate
library(survival) 
KM0 <- survfit(Surv(AmountPaid, UnCensored) ~ 1, type="kaplan-meier", data=??)

# plot function survival function
plot(??, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")
```


```{r ex="LDA1.4.3.2_1", type="solution", tut=TRUE}

# Add policy limit for positive loss data
positive_loss$PolicyLimit <- 100000
positive_loss$PolicyLimit[1:100] <- 50000

# Add AmountPaid column: using dplyr 
library(dplyr)
positive_loss_limit <- positive_loss %>% rowwise() %>% mutate(AmountPaid=min(y, PolicyLimit))

# Add UnCensored column: traditional method
positive_loss_limit$UnCensored <- 1*(positive_loss_limit$AmountPaid < positive_loss_limit$PolicyLimit)

# KM estimate
library(survival) 
KM0 <- survfit(Surv(AmountPaid, UnCensored) ~ 1, type="kaplan-meier", data=positive_loss_limit)

# plot function survival function
plot(KM0, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")

```


```{r ex="LDA1.4.3.2_1", type="sct", tut=TRUE}

positive_lossmsg <- "Did you correctly specify the object `positive_loss`?"
ex() %>% check_object("positive_loss", undefined_msg = "Make sure to not remove `positive_loss`!") %>% check_equal(incorrect_msg=positive_lossmsg)

KM0msg <- "Did you correctly specify the object `KM0`?"
ex() %>% check_object("KM0", undefined_msg = "Make sure to not remove `KM0`!") %>% check_equal(incorrect_msg=KM0msg)

plot4msg<-"Use `plot` on the `KM0` object"
ex() %>% check_function("plot", not_called_msg=plot4msg) %>% check_result() %>% check_equal(incorrect_msg=plot4msg)

success_msg("Excellent! Handling data is an important aspect of the actuarial function. There are many ways to do data manipulation. You will find the flexibility afforded by dplyr package to be terrific as you become familiar with it.")
```

### Exercise. Nonparametric Estimation using Truncated and Censored Information

As described in [Section 4.3.2.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#nonparametric-estimation-using-modified-data), it is very common to have data subject to both deductibles and upper limits. As emphasized in the previous exercise, upper limits typically means that the data are censored from the right. Typically, in the presence of deductibles, the data are truncated from the left (that is, we usually don't see a claim less than the deductible). We make that assumption for this exercise, using the data from Exercise \@ref(Ex:KMEstimation) where the deductible variable is called `Deduct`.

With these modified data, we again estimate the survival distribution. The following code guides you through the estimation using both the  Kaplan-Meier Product Limit  and the Nelson-Äalen estimators. These methods are described in more detail in [Section 4.3.2.2 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#nonparametric-estimation-using-modified-data).


:::: {.blackbox }
**Instructions**

-  Calculate and plot the KM estimate of the survival function without deductible information, that is, as in the prior exercise.
-  Calculate and plot the KM estimate with deductible information.
-  Calculate and plot the Nelson-Äalen estimate of the survival function without deductible information.
-  Calculate and plot the Nelson-Äalen estimate with deductible information.

:::: 

<br>

```{r ex="LDA1.4.3.2_2", type="hint", tut=TRUE}

Take some time to explore the online `R` documentation.

Use the function Surv() to create the empirical distribution.

```


```{r ex="LDA1.4.3.2_2", type="pre-exercise-code", tut=TRUE}

library(MASS)
library(dplyr)
library(survival) 

Insample <- read.csv("https://raw.githubusercontent.com/OpenActTexts/LDACourse1/main/Data/Insample.csv", header=T, na.strings=c("."), stringsAsFactors=FALSE)

# Subset for positive loss
positive_loss <- subset(Insample, y>0)
positive_loss <- subset(positive_loss, Deduct <= y)

# Add policy limit for positive loss data
positive_loss$PolicyLimit <- 100000
positive_loss$PolicyLimit[1:100] <- 50000

# Add actual payment column: using dplyr 
positive_loss_limit <- positive_loss %>% rowwise() %>% mutate(AmountPaid=min(y, PolicyLimit)+.01)

# Add UnCensored column: traditional method
positive_loss_limit$UnCensored <- 1*(positive_loss_limit$AmountPaid < positive_loss_limit$PolicyLimit)

```


```{r ex="LDA1.4.3.2_2", type="sample-code", tut=TRUE}
library(survival)
# Calculate and plot the KM estimate without deductible information
KM0 <- survfit(??, type="kaplan-meier", data=positive_loss_limit)
plot(KM0, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")

# Calculate and plot the KM estimate with deductible information
KM1 <- survfit(Surv(Deduct, AmountPaid, UnCensored) ~ 1, type="kaplan-meier", data=??)
plot(??, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")

# Calculate and plot the Nelson-Aalen estimate without deductible information
NA0 <- survfit(??, type="fleming-harrington", data=??)
plot(??, conf.int=FALSE, xlab="x", ylab="Nelson-Aalen Survival")

# Calculate and plot the Nelson-Aalen estimate with deductible information
NA1 <- survfit(?? ~ 1, type="fleming-harrington", data=??)
plot(??, conf.int=FALSE, xlab="x", ylab="Nelson-Aalen Survival")

```


```{r ex="LDA1.4.3.2_2", type="solution", tut=TRUE}
library(survival)
# Calculate and plot the KM estimate without deductible information
KM0 <- survfit(Surv(AmountPaid, UnCensored) ~ 1, type="kaplan-meier", data=positive_loss_limit)
plot(KM0, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")

# Calculate and plot the KM estimate with deductible information
KM1 <- survfit(Surv(Deduct, AmountPaid, UnCensored) ~ 1, type="kaplan-meier", data=positive_loss_limit)
plot(KM1, conf.int=FALSE, xlab="x", ylab="Kaplan Meier Survival")

# Calculate and plot the Nelson-Aalen estimate without deductible information
NA0 <- survfit(Surv(AmountPaid, UnCensored) ~ 1, type="fleming-harrington", data=positive_loss_limit)
plot(NA0, conf.int=FALSE, xlab="x", ylab="Nelson-Aalen Survival")

# Calculate and plot the Nelson-Aalen estimate with deductible information
NA1 <- survfit(Surv(Deduct, AmountPaid, UnCensored) ~ 1, type="fleming-harrington", data=positive_loss_limit)
plot(NA1, conf.int=FALSE, xlab="x", ylab="Nelson-Aalen Survival")

```


```{r ex="LDA1.4.3.2_2", type="sct", tut=TRUE}
KM0msg <- "Did you correctly specify the object `KM0`?"
ex() %>% check_object("KM0", undefined_msg = "Make sure to not remove `KM0`!") %>% check_equal(incorrect_msg=KM0msg)

KM1msg <- "Did you correctly specify the object `KM1`?"
ex() %>% check_object("KM1", undefined_msg = "Make sure to not remove `KM1`!") %>% check_equal(incorrect_msg=KM1msg)

NA0msg <- "Did you correctly specify the object `NA0`?"
ex() %>% check_object("NA0", undefined_msg = "Make sure to not remove `NA0`!") %>% check_equal(incorrect_msg=NA0msg)

NA1msg <- "Did you correctly specify the object `NA1`?"
ex() %>% check_object("NA1", undefined_msg = "Make sure to not remove `NA1`!") %>% check_equal(incorrect_msg=NA1msg)

success_msg("Excellent!  Now we can see how the policy limit and deductible information change estimates of a survival curve.")
```
  

## Estimation using Modified Data: Parametric Approach

***

In this section, you learn how to:

-  Describe grouped and censored truncated data.
-  Estimate parametric distributions based on grouped, censored,and truncated data.

***

####  Video: Parametric Estimation using Modified Data {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_sfh9ihwy&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_90krq256" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Parametric Estimation using Modified Data (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect432A')">A. Truncated Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect432B')">B. Maximum Likelihood Estimation with Grouped Data</button>
  <button class="tablinks" onclick="openTab(event, 'Sect432C')">C. Censored Data Likelihood I</button>
  <button class="tablinks" onclick="openTab(event, 'Sect432D')">D. Censored Data Likelihood II</button>
  <button class="tablinks" onclick="openTab(event, 'Sect432E')">E. Maximum Likelihood Estimation Using Censored and Truncated Data I</button>
  <button class="tablinks" onclick="openTab(event, 'Sect432F')">F. Maximum Likelihood Estimation Using Censored and Truncated Data II</button>
  </div>

<div id="Sect432A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=36" width="100%" height="400"> </iframe>
  </div>
<div id="Sect432B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=37" width="100%" height="400"> </iframe>
  </div>
<div id="Sect432C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=38" width="100%" height="400"> </iframe>
  </div>
<div id="Sect432D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=39" width="100%" height="400"> </iframe>
  </div>
<div id="Sect432E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=40" width="100%" height="400"> </iframe>
  </div>  
<div id="Sect432F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=41" width="100%" height="400"> </iframe>
  </div>  


<!-- ```{r child = './Quizzes/Quiz43.html', eval = QUIZ} -->
<!-- ``` -->

## Bayesian Inference

***

In this section, you learn how to:

-  Describe the Bayesian model as an alternative to the frequentist approach and summarize the five components of this modeling approach.
-  Summarize posterior distributions of parameters and use these posterior distributions to predict new outcomes.
-  Use conjugate distributions to determine posterior distributions of parameters.

***

####  Video: Bayesian Inference {-}

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298521/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_ko3jc8el&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_vo11odg9" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

#### Overheads: Bayesian Inference (Click Tab to View) {-}


<div class="tab">
  <button class="tablinks" onclick="openTab(event, 'Sect44A')">A. Bayesian Inference</button>
  <button class="tablinks" onclick="openTab(event, 'Sect44B')">B. Bayesian Inference Strengths</button>
  <button class="tablinks" onclick="openTab(event, 'Sect44C')">C. Bayesian Model I</button>
  <button class="tablinks" onclick="openTab(event, 'Sect44D')">D. Bayesian Model II</button>
  <button class="tablinks" onclick="openTab(event, 'Sect44E')">E. Posterior Computation</button>
  <button class="tablinks" onclick="openTab(event, 'Sect44F')">F. Poisson–Gamma Conjugate Family</button>
  </div>

<div id="Sect44A" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=44" width="100%" height="400"> </iframe>
  </div>
<div id="Sect44B" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=45" width="100%" height="400"> </iframe>
  </div>
<div id="Sect44C" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=46" width="100%" height="400"> </iframe>
  </div>
<div id="Sect44D" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=47" width="100%" height="400"> </iframe>
  </div>
<div id="Sect44E" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=48" width="100%" height="400"> </iframe>
  </div>  
<div id="Sect44F" class="tabcontent">
  <span onclick="this.parentElement.style.display='none'" class="topright">Hide</span>
  <iframe src="./Overheads/LDA1.Chap4.pdf#page=49" width="100%" height="400"> </iframe>
  </div>
  

### Visualizing Bayesian Methods

<center>

<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/1660902/sp/166090200/embedIframeJs/uiconf_id/34298531/partner_id/1660902?iframeembed=true&playerId=kaltura_player&entry_id=1_yv1ioaxl&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en_US&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_8j9zvdbn" width="649" height="401" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *"  frameborder="0" title="Kaltura Player"></iframe>

</center>

**Assignment Text**

In this tutorial, we will visually explore the prior and posterior distributions from [Example 4.4.1 of *Loss Data Analytics*](https://openacttexts.github.io/Loss-Data-Analytics/C-ModelSelection.html#bayesian-inference), and explore what would happen if we observed one more observation as well.  Recall that for this problem, the prior distribution was 
\[
\pi(q) = q^3/0.07, \hspace{5mm} 0.6 \leq q \leq 0.8.
\]

two observations from a Bernoulli(q) were $X_1 = 1, X_2 = 0$, and the posterior was found (Example 4.4.1) to be
\[
\pi(q \mid 1,0) = \frac{q^4 - q^5}{0.014069}.
\]

:::: {.blackbox }
**Instructions** 

- Make a [plot()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/plot) of the prior distribution.
- Think about how you expect the observed data to reshape it to the posterior.
- Add the posterior and a legend to your plot, to confirm the influence of the observed data.  This requires advanced commands such as [lines()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/lines) and [legend()](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/legend).
::::

<br>

```{r ex="LDA4.4.1.1", type="sample-code", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
prior = ??
plot(?? , ??, col="red", type="l", ylab="")
```

```{r ex="LDA4.4.1.1", type="hint", tut=TRUE}
Recall the mathematical definition of the prior density function.  Plot this against the sequence of parameter values. 
```

```{r ex="LDA4.4.1.1", type="solution", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
prior = q^3/0.07
plot(q, prior, col="red", type="l", ylab="")
```

```{r ex="LDA4.4.1.1", type="sct", tut=TRUE}

priormsg <- "Did you correctly specify the object `prior`?"
ex() %>% check_object("prior", undefined_msg = "Make sure to not remove `prior`!") %>% check_equal(incorrect_msg=priormsg)

plot5msg<-"Use `plot` on the `q and `prior` objects"
ex() %>% check_function("plot", not_called_msg=plot5msg) %>% check_result() %>% check_equal(incorrect_msg=plot5msg)

success_msg("Excellent!")
```

Notice that the prior distribution sets the support for $q$, which is bounded between 0.6 and 0.8.  The posterior will necessarily have this same support.  Notice also that the prior distribution places more mass for larger values of $q$.  This is saying that prior to observing data, we think larger parameters values near 0.8 are more likely than smaller values near 0.6

The observed data has a claim in year 1 ($X_1 = 1$) and no claim in year 2 ($X_2 = 0$).  So, we have two independent samples drawn from a Bernoulli(q), and $\overline{X} = 0.5$.  If we were only using the likelihood and a method such as maximum likelihood estimation, we would estimate $\widehat{q} = \overline{X} = 0.5$.  So these data are telling us that values near $q=0.5$ are consistent.  But, Bayesian methods blend information from the data with information from the prior to produce a posterior.  So, expect that our posterior distribution will place more mass towards smaller values of $q$, and less towards larger values.  Let's see if that is true.


```{r ex="LDA4.4.1.2", type="hint", tut=TRUE}
Make sure you understand which is the prior density and which is the posterior.  Also, make sure you are in the habit of keeping colors and line types consistent in your work.
```

```{r ex="LDA4.4.1.2", type="sample-code", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
?? = q^3/0.07
posterior = ??

plot(q, prior, col="red", type="l", ylab="")
lines(q, ??, col="blue", type="l", lty=2)
legend("topleft",legend=c("Prior","Posterior"), col=??, lty=c(1,2))
```


```{r ex="LDA4.4.1.2", type="solution", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
prior = q^3/0.07
posterior = (q^4 - q^5)/0.014069

plot(q, prior, col="red", type="l", ylab="")
lines(q, posterior, col="blue", type="l", lty=2)
legend("topleft",legend=c("Prior","Posterior"), col=c("red","blue"), lty=c(1,2))
```

```{r ex="LDA4.4.1.2", type="sct", tut=TRUE}
prior1msg <- "Did you correctly specify the object `prior`?"
ex() %>% check_object("prior", undefined_msg = "Make sure to not remove `prior`!") %>% check_equal(incorrect_msg=prior1msg)

posteriormsg <- "Did you correctly specify the object `posterior`?"
ex() %>% check_object("posterior", undefined_msg = "Make sure to not remove `posterior`!") %>% check_equal(incorrect_msg=posteriormsg)

plot6msg<-"Use `plot` on the `q and `prior` objects"
ex() %>% check_function("plot", not_called_msg=plot6msg) %>% check_result() %>% check_equal(incorrect_msg=plot6msg)

success_msg("Excellent!  Now we can see how the observed data shifted our understanding of this unknown parameter.")
```

Indeed, the posterior has shifted the mass towards lower values, balancing what the data tell us and what the prior tells us.  Since this is only two data points, the shift is not especially dramatic. 

**Assignment Extension**

Let's continue with the prior example, but now imagine we next observe the third year, and there was again no claim, $X_3 = 0$.  We will compute the new posterior distribution, and compare it to the posterior in the previous problem.

:::: {.blackbox }
**Instructions** 

- Make a plot showing the original prior, posterior from example 4.1.1, and posterior for the new case.
::::

<br>

The posterior distribution after observing three data points is:

$$
\pi(q \mid 1,0,0) \propto f(1\mid q)f(0\mid q)f(0\mid q)\pi(q) = q(1-q)(1-q)q^3 = q^4 - 2q^5 + q^6
$$

Recall the posterior is proportional to $q^4 - 2q^5 + q^6$, so that means
\[
\pi(q \mid 1,0,0) = c \left(q^4 - 2q^5 + q^6 \right).
\]

In order to find the normalizing constant $c$, we would need to integrate this and make sure the integral is 1:

\[
\begin{array}{ll}
1 &= \int_{q=0.6}^{0.8} c \left(q^4 - 2q^5 + q^6\right)dq\\
 &= c \left(\frac{1}{5}q^5 - \frac{2}{6} q^6 +\frac{1}{7} q^7 \right) \rvert_{q=0.6}^{q=0.8}\\
 &= c \left( 0.0081 - 0.00400 \right)\\
 &= c \left(0.0041 \right)\\ 
 \end{array}
\]

Thus, we see the normalizing constant is $c = \frac{1}{.0041}$.  We can now revisit the code and add the new posterior line:

```{r ex="LDA4.4.1.3", type="sample-code", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
prior = q^3/0.07
posterior_old = (q^4 - q^5)/0.014069
posterior_new = ??

plot(q, prior, col="red", type="l", ylab="")
lines(q, posterior_old, col="blue", type="l", lty=2)
lines(q, posterior_new, col="green", type="l", lty=3)
legend("topleft",legend=c("Prior","Old Posterior","New Posterior"), col=c("red","blue",??), lty=c(1,2,3))
```

```{r ex="LDA4.4.1.3", type="solution", tut=TRUE}
q = seq(0.6, 0.8, by = .01)
prior = q^3/0.07
posterior_old = (q^4 - q^5)/0.014069
posterior_new = (q^4 - 2*q^5+q^6)/0.0041

plot(q, prior, col="red", type="l", ylab="")
lines(q, posterior_old, col="blue", type="l", lty=2)
lines(q, posterior_new, col="green", type="l", lty=3)
legend("topleft",legend=c("Prior","Old Posterior","New Posterior"), col=c("red","blue","green"), lty=c(1,2,3))
```
As expected, the new posterior has shifted even more mass towards lower parameter values in the posterior, since $\overline{X} = \frac{1}{3}$ and the data are more consistent with low parameter values for $q$.


```{r ex="LDA4.4.1.3", type="sct", tut=TRUE}
prior1msg <- "Did you correctly specify the object `prior`?"
ex() %>% check_object("prior", undefined_msg = "Make sure to not remove `prior`!") %>% check_equal(incorrect_msg=prior1msg)

posteriornewmsg <- "Did you correctly specify the object `posterior_new`?"
ex() %>% check_object("posterior_new", undefined_msg = "Make sure to not remove `posterior_new`!") %>% check_equal(incorrect_msg=posteriornewmsg)

plot7msg<-"Use `plot` on the `q and `prior` objects"
ex() %>% check_function("plot", not_called_msg=plot7msg) %>% check_result() %>% check_equal(incorrect_msg=plot7msg)

success_msg("Nice!")
```
  

## Contributors {-}

-  **Authors**. **Rob Erhardt**, Wake Forest University, **Brian Hartman**, Brigham Young University, and **Zhiyu (Frank) Quan**, University of Illinois at Urbana-Champaign, are the principal authors of the initial version of this chapter.
-  **Chapter Maintainers**. Please contact Frank at <zquan@illinois.edu> and/or Jed at <jfrees@bus.wisc.edu> for chapter comments and suggested improvements.

